{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMSeq2SeqAttn 60d — Ablation A\n",
    "\n",
    "### 목적: Rainfall 원복 (누적) + Dropout 유지 → rainfall 차분 변환이 성능 하락 원인인지 검증\n",
    "\n",
    "| 변경 항목 | v1 (원본) | v2 | 이 실험 (A) |\n",
    "|----------|:---------:|:--:|:---------:|\n",
    "| Rainfall | 누적 | 차분 | **누적 (v1 원복)** |\n",
    "| Decoder Dropout | 없음 | 0.2 | 0.2 (v2 유지) |\n",
    "| LayerNorm 순서 | LN→Drop→FC | Drop→LN→FC | Drop→LN→FC (v2) |\n",
    "| EarlyStopping patience | 5 | 10 | 10 |\n",
    "| Multi-run (N=3) | ✗ | ✓ | ✓ |\n",
    "| pin_memory | ✗ | ✓ | ✓ |\n",
    "| 평가 지표 | MAPE only | +SMAPE,macro | +SMAPE,macro |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. flow 전처리 (IQR, Savgol)\n",
    "1. weather 전처리 (**누적 (v1 원복)**)\n",
    "2. flow & weather merge\n",
    "3~5. sliding window → split → 정규화\n",
    "6. Model (0.2 (v2 유지))\n",
    "7~8. 학습 (patience=10, N=3) → 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-run: 3 runs, seeds=[42, 123, 7]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, os, copy\n",
    "\n",
    "N_RUNS = 3\n",
    "SEEDS = [42, 123, 7]\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import torch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "print(f\"Multi-run: {N_RUNS} runs, seeds={SEEDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mpiqw98v9mq",
   "metadata": {},
   "source": [
    "### Data Split 구조 (4계절 Block Sampling, 60일 x 4)\n",
    "\n",
    "각 블록(60일)을 시간 순서대로 70 / 15 / 15로 분할 (블록 간 87분 gap으로 data leak 방지)\n",
    "\n",
    "| 계절 | Train (~42일) | Val (~9일) | Test (~9일) |\n",
    "|------|--------------|-------------|--------------|\n",
    "| Winter | 23/12/01 ~ 24/01/11 | 24/01/11 ~ 24/01/20 | 24/01/20 ~ 24/01/29 |\n",
    "| Spring | 24/03/15 ~ 24/04/25 | 24/04/25 ~ 24/05/04 | 24/05/04 ~ 24/05/13 |\n",
    "| Summer | 23/07/01 ~ 23/08/11 | 23/08/11 ~ 23/08/20 | 23/08/20 ~ 23/08/29 |\n",
    "| Fall | 23/09/15 ~ 23/10/26 | 23/10/26 ~ 23/11/04 | 23/11/04 ~ 23/11/13 |\n",
    "\n",
    "**예측 구조:** 72분 입력 (10 features) -> 15분 예측 (flow value) x 4 rolling = 1시간\n",
    "\n",
    "**모델:** LSTM Seq2Seq + **Bahdanau Attention** — 전체 Hidden Sequence 활용\n",
    "- **기존 Cross-Attention 실패 원인:**\n",
    "  - Query = h₇₂ (고정) → 모든 step이 동일한 가중치(1/72) → Attention 무효\n",
    "- **Bahdanau Attention 해결:**\n",
    "  - Query = **Decoder의 현재 step hidden state** (step마다 다름)\n",
    "  - step 1의 Query ≠ step 15의 Query → step별로 다른 곳을 참조\n",
    "- **Model B(Autoreg) 실패와의 차이:**\n",
    "  - Model B: prev_pred 피드백 → error accumulation + Scheduled Sampling 필요\n",
    "  - **이 모델: step_embed 입력 → error 격리 + Teacher forcing 불필요**\n",
    "\n",
    "**Loss:** Step-weighted MSE (step 1=1.0 -> step 15=2.0)\n",
    "**LR Scheduler:** ReduceLROnPlateau (factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef375de",
   "metadata": {},
   "source": [
    "0. flow, weather 데이터 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e49d44e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:28.442134Z",
     "iopub.status.busy": "2026-02-09T08:31:28.441903Z",
     "iopub.status.idle": "2026-02-09T08:31:29.231872Z",
     "shell.execute_reply": "2026-02-09T08:31:29.230864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow 원본: 943,434개 (2023-01-01 00:01:00 ~ 2024-10-17 17:19:00)\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path.cwd().parent\n",
    "\n",
    "# Flow raw data 로드 (J배수지, reservoir/10.csv)\n",
    "flow_file = BASE_DIR / \"data\" / \"rawdata\" / \"reservoir\" / \"10.csv\"\n",
    "df_flow = pd.read_csv(\n",
    "    flow_file,\n",
    "    header=None,\n",
    "    usecols=[1, 2],\n",
    "    names=['time', 'value']\n",
    ").sort_values('time').reset_index(drop=True)\n",
    "\n",
    "df_flow['time'] = pd.to_datetime(df_flow['time'], errors='coerce')\n",
    "df_flow = df_flow.dropna(subset=['time'])\n",
    "print(f\"Flow 원본: {len(df_flow):,}개 ({df_flow['time'].min()} ~ {df_flow['time'].max()})\")\n",
    "\n",
    "# Weather 파일 경로\n",
    "weather_file = BASE_DIR / \"data\" / \"rawdata\" / \"weather\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fmpjlrux5n",
   "metadata": {},
   "source": [
    "0-1. Flow 전처리\n",
    "- IQR 이상치 + 음수 + 급변동 제거 → NaN\n",
    "- Linear interpolation (양방향)\n",
    "- Savitzky-Golay filter (window=51, polyorder=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "omjayxm4sj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이상치 제거: IQR=21, 음수=0, 급변동(>91.06)=944, 총=962\n",
      "전처리 완료: 943,434개, 범위 [0.00, 325.52]\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# 1. IQR 이상치 + 음수 + 급변동 → NaN 처리\n",
    "Q1 = df_flow['value'].quantile(0.25)\n",
    "Q3 = df_flow['value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "iqr_mask = (df_flow['value'] < Q1 - 1.5 * IQR) | (df_flow['value'] > Q3 + 1.5 * IQR)\n",
    "negative_mask = df_flow['value'] < 0\n",
    "diff = df_flow['value'].diff().abs()\n",
    "spike_threshold = diff.quantile(0.999)\n",
    "spike_mask = diff > spike_threshold\n",
    "\n",
    "total_mask = iqr_mask | negative_mask | spike_mask\n",
    "df_flow.loc[total_mask, 'value'] = np.nan\n",
    "\n",
    "print(f\"이상치 제거: IQR={iqr_mask.sum()}, 음수={negative_mask.sum()}, \"\n",
    "      f\"급변동(>{spike_threshold:.2f})={spike_mask.sum()}, 총={total_mask.sum()}\")\n",
    "\n",
    "# 2. Linear interpolation (양방향) + clip\n",
    "df_flow['value'] = df_flow['value'].interpolate(method='linear', limit_direction='both')\n",
    "df_flow['value'] = df_flow['value'].clip(lower=0)\n",
    "\n",
    "# 3. Savitzky-Golay filter (노이즈 제거, window=51, polyorder=2)\n",
    "df_flow['value'] = savgol_filter(df_flow['value'], window_length=51, polyorder=2)\n",
    "df_flow['value'] = df_flow['value'].clip(lower=0)\n",
    "\n",
    "print(f\"전처리 완료: {len(df_flow):,}개, \"\n",
    "      f\"범위 [{df_flow['value'].min():.2f}, {df_flow['value'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "n9muuusx0ga",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: /home/kp/web/work/pro/data/processed/flow_preprocessed.csv (943,434행)\n"
     ]
    }
   ],
   "source": [
    "# 전처리 결과 저장 (원본 보호 + 재현성)\n",
    "save_path = BASE_DIR / \"data\" / \"processed\" / \"flow_preprocessed.csv\"\n",
    "df_flow.to_csv(save_path, index=False)\n",
    "print(f\"저장 완료: {save_path} ({len(df_flow):,}행)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42037477",
   "metadata": {},
   "source": [
    "- weather 데이터 인코딩, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6eac53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:29.309590Z",
     "iopub.status.busy": "2026-02-09T08:31:29.309370Z",
     "iopub.status.idle": "2026-02-09T08:31:29.857621Z",
     "shell.execute_reply": "2026-02-09T08:31:29.856158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "읽을 파일 수: 23\n",
      "concat 후 행 수: 1,885,020\n"
     ]
    }
   ],
   "source": [
    "def read_weather_csv(f):\n",
    "    for enc in [\"euc-kr\", \"cp949\", \"utf-8\", \"utf-8-sig\"]:\n",
    "        try:\n",
    "            return pd.read_csv(f, encoding=enc)\n",
    "        except (UnicodeDecodeError, UnicodeError):\n",
    "            continue\n",
    "    raise ValueError(f\"인코딩 실패: {f.name}\")\n",
    "\n",
    "files = sorted(weather_file.glob(\"*.csv\"))\n",
    "print(f\"읽을 파일 수: {len(files)}\")\n",
    "\n",
    "weather = pd.concat(\n",
    "    [read_weather_csv(f) for f in files],\n",
    "    ignore_index=True\n",
    ")\n",
    "print(f\"concat 후 행 수: {len(weather):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0dd991",
   "metadata": {},
   "source": [
    "- 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dbc7f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:29.859881Z",
     "iopub.status.busy": "2026-02-09T08:31:29.859672Z",
     "iopub.status.idle": "2026-02-09T08:31:31.569575Z",
     "shell.execute_reply": "2026-02-09T08:31:31.568243Z"
    }
   },
   "outputs": [],
   "source": [
    "# weather 전체 concat 결과 저장 (원본 보호)\n",
    "weather.to_csv(BASE_DIR / \"data\" / \"processed\" / \"weather_total_raw.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. weather 전처리 (★ Rainfall: 원본 누적 유지 — v1 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "장기 결측 제거: 1,885,020 -> 1,788,078 (94.9% 유지)\n",
      "연속 세그먼트 수: 3564\n",
      "Rainfall: 원본 누적 강수량 유지 (v1 동일)\n",
      "(894039, 5)\n",
      "             datetime  temperature  rainfall  humidity  segment_id\n",
      "0 2023-01-01 00:01:00         -3.3       0.0      91.6           0\n",
      "1 2023-01-01 00:02:00         -3.3       0.0      91.6           0\n",
      "2 2023-01-01 00:03:00         -3.3       0.0      91.6           0\n",
      "3 2023-01-01 00:04:00         -3.2       0.0      91.6           0\n",
      "4 2023-01-01 00:05:00         -3.2       0.0      91.6           0\n"
     ]
    }
   ],
   "source": [
    "weather['일시'] = pd.to_datetime(weather['일시'])\n",
    "\n",
    "# 결측 처리: 강수량 0 채움, 기온/습도 linear 보간 (최대 60분 gap)\n",
    "weather['0.5mm 일 누적 강수량(mm)'] = weather['0.5mm 일 누적 강수량(mm)'].fillna(0)\n",
    "weather['기온(℃)'] = weather['기온(℃)'].interpolate(method='linear', limit=60)\n",
    "weather['상대습도(%)'] = weather['상대습도(%)'].interpolate(method='linear', limit=60)\n",
    "\n",
    "# 장기 결측(>60분) 행 제거\n",
    "before = len(weather)\n",
    "weather = weather.dropna(subset=['기온(℃)', '상대습도(%)'])\n",
    "print(f'장기 결측 제거: {before:,} -> {len(weather):,} ({len(weather)/before*100:.1f}% 유지)')\n",
    "\n",
    "weather = weather.rename(columns={\n",
    "    '일시': 'datetime',\n",
    "    '기온(℃)': 'temperature',\n",
    "    '0.5mm 일 누적 강수량(mm)': 'rainfall',\n",
    "    '상대습도(%)': 'humidity',\n",
    "})\n",
    "\n",
    "# 중복 timestamp 제거\n",
    "weather = weather.drop_duplicates(subset='datetime', keep='first')\n",
    "weather = weather.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# 시간 불연속 경계 표시\n",
    "time_diff = weather['datetime'].diff()\n",
    "seg_boundary = time_diff > pd.Timedelta(minutes=1)\n",
    "weather['segment_id'] = seg_boundary.cumsum()\n",
    "print(f'연속 세그먼트 수: {weather[\"segment_id\"].nunique()}')\n",
    "print(f\"Rainfall: 원본 누적 강수량 유지 (v1 동일)\")\n",
    "print(weather.shape)\n",
    "print(weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed201ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:31.929338Z",
     "iopub.status.busy": "2026-02-09T08:31:31.929140Z",
     "iopub.status.idle": "2026-02-09T08:31:32.034507Z",
     "shell.execute_reply": "2026-02-09T08:31:32.033286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>humidity</th>\n",
       "      <th>segment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>894039</td>\n",
       "      <td>894039.000000</td>\n",
       "      <td>894039.000000</td>\n",
       "      <td>894039.000000</td>\n",
       "      <td>894039.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2023-12-09 07:18:45.749011</td>\n",
       "      <td>15.358957</td>\n",
       "      <td>1.922265</td>\n",
       "      <td>71.807508</td>\n",
       "      <td>1596.019969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2023-01-01 00:01:00</td>\n",
       "      <td>-16.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2023-07-03 09:58:30</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.200000</td>\n",
       "      <td>465.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2023-12-13 20:09:00</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2024-05-25 11:14:30</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.300000</td>\n",
       "      <td>3267.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-11-01 00:00:00</td>\n",
       "      <td>37.700000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>99.900000</td>\n",
       "      <td>3563.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.908432</td>\n",
       "      <td>8.737307</td>\n",
       "      <td>21.188111</td>\n",
       "      <td>1320.075920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         datetime    temperature       rainfall  \\\n",
       "count                      894039  894039.000000  894039.000000   \n",
       "mean   2023-12-09 07:18:45.749011      15.358957       1.922265   \n",
       "min           2023-01-01 00:01:00     -16.500000       0.000000   \n",
       "25%           2023-07-03 09:58:30       6.300000       0.000000   \n",
       "50%           2023-12-13 20:09:00      16.500000       0.000000   \n",
       "75%           2024-05-25 11:14:30      24.700000       0.000000   \n",
       "max           2024-11-01 00:00:00      37.700000     160.000000   \n",
       "std                           NaN      10.908432       8.737307   \n",
       "\n",
       "            humidity     segment_id  \n",
       "count  894039.000000  894039.000000  \n",
       "mean       71.807508    1596.019969  \n",
       "min         2.900000       0.000000  \n",
       "25%        57.200000     465.000000  \n",
       "50%        76.000000    1066.000000  \n",
       "75%        90.300000    3267.000000  \n",
       "max        99.900000    3563.000000  \n",
       "std        21.188111    1320.075920  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b56b79a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:32.036640Z",
     "iopub.status.busy": "2026-02-09T08:31:32.036417Z",
     "iopub.status.idle": "2026-02-09T08:31:32.039770Z",
     "shell.execute_reply": "2026-02-09T08:31:32.038864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime       0\n",
       "temperature    0\n",
       "rainfall       0\n",
       "humidity       0\n",
       "segment_id     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(weather)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rainfall: 일 누적 강우량 → 원본 유지 (v1 동일) MinMaxScaler만 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b0f6d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:32.948750Z",
     "iopub.status.busy": "2026-02-09T08:31:32.948560Z",
     "iopub.status.idle": "2026-02-09T08:31:32.956793Z",
     "shell.execute_reply": "2026-02-09T08:31:32.955473Z"
    }
   },
   "outputs": [],
   "source": [
    "df_weather = df[['datetime', 'temperature', 'rainfall', 'humidity', 'segment_id']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8bbb2",
   "metadata": {},
   "source": [
    "2-1. weather data merge\n",
    "2-2. weather, time feature 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f7ded0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:33.072829Z",
     "iopub.status.busy": "2026-02-09T08:31:33.072617Z",
     "iopub.status.idle": "2026-02-09T08:31:33.125952Z",
     "shell.execute_reply": "2026-02-09T08:31:33.124479Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_flow, df_weather, how='inner', left_on='time', right_on=\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0b5e38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:33.147654Z",
     "iopub.status.busy": "2026-02-09T08:31:33.147472Z",
     "iopub.status.idle": "2026-02-09T08:31:33.151697Z",
     "shell.execute_reply": "2026-02-09T08:31:33.150351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터: 872,723개 (1분 단위)\n",
      "merge 후 연속 세그먼트 수: 4158\n",
      "Temporal features 추가 완료: (872723, 12)\n",
      "                 time      value  temperature  rainfall  humidity  segment_id  \\\n",
      "0 2023-01-01 00:01:00  96.577302         -3.3       0.0      91.6           0   \n",
      "1 2023-01-01 00:02:00  96.739744         -3.3       0.0      91.6           0   \n",
      "2 2023-01-01 00:03:00  96.895855         -3.3       0.0      91.6           0   \n",
      "3 2023-01-01 00:04:00  97.045636         -3.2       0.0      91.6           0   \n",
      "4 2023-01-01 00:05:00  97.189086         -3.2       0.0      91.6           0   \n",
      "\n",
      "   time_sin  time_cos   dow_sin   dow_cos  season_sin  season_cos  \n",
      "0  0.502182  0.999995  0.109084  0.811745    0.508601    0.999926  \n",
      "1  0.504363  0.999981  0.109084  0.811745    0.508601    0.999926  \n",
      "2  0.506545  0.999957  0.109084  0.811745    0.508601    0.999926  \n",
      "3  0.508726  0.999924  0.109084  0.811745    0.508601    0.999926  \n",
      "4  0.510907  0.999881  0.109084  0.811745    0.508601    0.999926  \n"
     ]
    }
   ],
   "source": [
    "df_merged = df_merged.drop(columns=['datetime'])\n",
    "\n",
    "# 1분 단위 유지 (리샘플링 없음)\n",
    "df_merged = df_merged.sort_values('time').reset_index(drop=True)\n",
    "df_merged = df_merged.dropna()\n",
    "print(f\"데이터: {len(df_merged):,}개 (1분 단위)\")\n",
    "\n",
    "# ★ merge 후 시간 불연속 경계 재계산 (weather segment_id 대체)\n",
    "# weather의 segment_id는 weather 단독 기준이므로,\n",
    "# flow+weather inner merge 후 실제 불연속을 반영해야 정확함\n",
    "time_diff_merged = df_merged['time'].diff()\n",
    "seg_boundary_merged = time_diff_merged > pd.Timedelta(minutes=1)\n",
    "df_merged['segment_id'] = seg_boundary_merged.cumsum()\n",
    "print(f\"merge 후 연속 세그먼트 수: {df_merged['segment_id'].nunique()}\")\n",
    "\n",
    "# Cyclical temporal features\n",
    "t: pd.Series = df_merged['time']\n",
    "\n",
    "# int -> float cast for arithmetic with np.pi\n",
    "hour = t.dt.hour.astype(np.float64)\n",
    "minute = t.dt.minute.astype(np.float64)\n",
    "dow = t.dt.dayofweek.astype(np.float64)\n",
    "doy = t.dt.dayofyear.astype(np.float64)\n",
    "\n",
    "# 시간정보 (분 단위 하루 주기, T=1440)\n",
    "minute_of_day = hour * 60 + minute\n",
    "df_merged['time_sin'] = 0.5 * np.sin(2 * np.pi * minute_of_day / 1440) + 0.5\n",
    "df_merged['time_cos'] = 0.5 * np.cos(2 * np.pi * minute_of_day / 1440) + 0.5\n",
    "\n",
    "# 요일 (주간 주기, T=7)\n",
    "df_merged['dow_sin'] = 0.5 * np.sin(2 * np.pi * dow / 7) + 0.5\n",
    "df_merged['dow_cos'] = 0.5 * np.cos(2 * np.pi * dow / 7) + 0.5\n",
    "\n",
    "# 계절 (연간 주기, T=365.25)\n",
    "df_merged['season_sin'] = 0.5 * np.sin(2 * np.pi * doy / 365.25) + 0.5\n",
    "df_merged['season_cos'] = 0.5 * np.cos(2 * np.pi * doy / 365.25) + 0.5\n",
    "\n",
    "print(f\"Temporal features 추가 완료: {df_merged.shape}\")\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ee3a4f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:33.164994Z",
     "iopub.status.busy": "2026-02-09T08:31:33.164815Z",
     "iopub.status.idle": "2026-02-09T08:31:33.295128Z",
     "shell.execute_reply": "2026-02-09T08:31:33.293831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 872723 entries, 0 to 872722\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   time         872723 non-null  datetime64[us]\n",
      " 1   value        872723 non-null  float64       \n",
      " 2   temperature  872723 non-null  float64       \n",
      " 3   rainfall     872723 non-null  float64       \n",
      " 4   humidity     872723 non-null  float64       \n",
      " 5   segment_id   872723 non-null  int64         \n",
      " 6   time_sin     872723 non-null  float64       \n",
      " 7   time_cos     872723 non-null  float64       \n",
      " 8   dow_sin      872723 non-null  float64       \n",
      " 9   dow_cos      872723 non-null  float64       \n",
      " 10  season_sin   872723 non-null  float64       \n",
      " 11  season_cos   872723 non-null  float64       \n",
      "dtypes: datetime64[us](1), float64(10), int64(1)\n",
      "memory usage: 79.9 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((872723, 12), None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape, df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2569d9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>value</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>humidity</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>time_sin</th>\n",
       "      <th>time_cos</th>\n",
       "      <th>dow_sin</th>\n",
       "      <th>dow_cos</th>\n",
       "      <th>season_sin</th>\n",
       "      <th>season_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>872723</td>\n",
       "      <td>872723.000000</td>\n",
       "      <td>872723.000000</td>\n",
       "      <td>872723.000000</td>\n",
       "      <td>872723.000000</td>\n",
       "      <td>872723.000000</td>\n",
       "      <td>872723.000000</td>\n",
       "      <td>872723.000000</td>\n",
       "      <td>872723.000000</td>\n",
       "      <td>872723.000000</td>\n",
       "      <td>8.727230e+05</td>\n",
       "      <td>872723.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2023-12-01 18:39:20.141650</td>\n",
       "      <td>151.786570</td>\n",
       "      <td>15.354982</td>\n",
       "      <td>1.932336</td>\n",
       "      <td>71.595445</td>\n",
       "      <td>1853.432576</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>0.498633</td>\n",
       "      <td>0.496760</td>\n",
       "      <td>0.498728</td>\n",
       "      <td>5.197144e-01</td>\n",
       "      <td>0.470201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2023-01-01 00:01:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012536</td>\n",
       "      <td>0.049516</td>\n",
       "      <td>2.889877e-07</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2023-06-27 11:00:30</td>\n",
       "      <td>109.030874</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.900000</td>\n",
       "      <td>613.000000</td>\n",
       "      <td>0.146447</td>\n",
       "      <td>0.144907</td>\n",
       "      <td>0.109084</td>\n",
       "      <td>0.049516</td>\n",
       "      <td>1.605898e-01</td>\n",
       "      <td>0.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2023-12-06 15:46:00</td>\n",
       "      <td>144.881767</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.700000</td>\n",
       "      <td>1311.000000</td>\n",
       "      <td>0.502182</td>\n",
       "      <td>0.497818</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.388740</td>\n",
       "      <td>5.600631e-01</td>\n",
       "      <td>0.457583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2024-05-14 15:26:30</td>\n",
       "      <td>192.717293</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>3509.000000</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.852007</td>\n",
       "      <td>0.890916</td>\n",
       "      <td>0.811745</td>\n",
       "      <td>8.732941e-01</td>\n",
       "      <td>0.812944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-17 17:19:00</td>\n",
       "      <td>325.516638</td>\n",
       "      <td>37.700000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>99.900000</td>\n",
       "      <td>4157.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.999928e-01</td>\n",
       "      <td>0.999926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>56.898401</td>\n",
       "      <td>11.013743</td>\n",
       "      <td>8.825355</td>\n",
       "      <td>21.235732</td>\n",
       "      <td>1468.650283</td>\n",
       "      <td>0.353499</td>\n",
       "      <td>0.353606</td>\n",
       "      <td>0.352849</td>\n",
       "      <td>0.354239</td>\n",
       "      <td>3.565895e-01</td>\n",
       "      <td>0.348665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             time          value    temperature  \\\n",
       "count                      872723  872723.000000  872723.000000   \n",
       "mean   2023-12-01 18:39:20.141650     151.786570      15.354982   \n",
       "min           2023-01-01 00:01:00       0.000000     -16.500000   \n",
       "25%           2023-06-27 11:00:30     109.030874       6.100000   \n",
       "50%           2023-12-06 15:46:00     144.881767      16.500000   \n",
       "75%           2024-05-14 15:26:30     192.717293      24.800000   \n",
       "max           2024-10-17 17:19:00     325.516638      37.700000   \n",
       "std                           NaN      56.898401      11.013743   \n",
       "\n",
       "            rainfall       humidity     segment_id       time_sin  \\\n",
       "count  872723.000000  872723.000000  872723.000000  872723.000000   \n",
       "mean        1.932336      71.595445    1853.432576       0.500568   \n",
       "min         0.000000       2.900000       0.000000       0.000000   \n",
       "25%         0.000000      56.900000     613.000000       0.146447   \n",
       "50%         0.000000      75.700000    1311.000000       0.502182   \n",
       "75%         0.000000      90.000000    3509.000000       0.853553   \n",
       "max       160.000000      99.900000    4157.000000       1.000000   \n",
       "std         8.825355      21.235732    1468.650283       0.353499   \n",
       "\n",
       "            time_cos        dow_sin        dow_cos    season_sin  \\\n",
       "count  872723.000000  872723.000000  872723.000000  8.727230e+05   \n",
       "mean        0.498633       0.496760       0.498728  5.197144e-01   \n",
       "min         0.000000       0.012536       0.049516  2.889877e-07   \n",
       "25%         0.144907       0.109084       0.049516  1.605898e-01   \n",
       "50%         0.497818       0.500000       0.388740  5.600631e-01   \n",
       "75%         0.852007       0.890916       0.811745  8.732941e-01   \n",
       "max         0.999995       0.987464       1.000000  9.999928e-01   \n",
       "std         0.353606       0.352849       0.354239  3.565895e-01   \n",
       "\n",
       "          season_cos  \n",
       "count  872723.000000  \n",
       "mean        0.470201  \n",
       "min         0.000010  \n",
       "25%         0.128500  \n",
       "50%         0.457583  \n",
       "75%         0.812944  \n",
       "max         0.999926  \n",
       "std         0.348665  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "qi3t6a7l9i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Winter] 2023-12-01 ~ 2024-01-29: 80,828 rows\n",
      "[Spring] 2024-03-15 ~ 2024-05-13: 78,794 rows\n",
      "[Summer] 2023-07-01 ~ 2023-08-29: 83,256 rows\n",
      "[Fall] 2023-09-15 ~ 2023-11-13: 80,430 rows\n",
      "\n",
      "Total: 323,308 rows (4-season block sampling, 60일 × 4)\n"
     ]
    }
   ],
   "source": [
    "# 2-3. 계절별 Block Sampling (60일 × 4계절, 2023-2024 교차)\n",
    "# 45일 대비 데이터 확대 효과 검증 (단일 변인: 기간만 변경)\n",
    "\n",
    "season_ranges = [\n",
    "    ('2023-12-01', '2024-01-29', 'Winter'),\n",
    "    ('2024-03-15', '2024-05-13', 'Spring'),\n",
    "    ('2023-07-01', '2023-08-29', 'Summer'),\n",
    "    ('2023-09-15', '2023-11-13', 'Fall'),\n",
    "]\n",
    "\n",
    "blocks = []\n",
    "for i, (start, end, name) in enumerate(season_ranges):\n",
    "    mask = (df_merged['time'] >= start) & (df_merged['time'] <= end)\n",
    "    block = df_merged[mask].copy()\n",
    "    block['block_id'] = i\n",
    "    blocks.append(block)\n",
    "    print(f\"[{name}] {start} ~ {end}: {len(block):,} rows\")\n",
    "\n",
    "df_merged = pd.concat(blocks, ignore_index=True)\n",
    "print(f\"\\nTotal: {len(df_merged):,} rows (4-season block sampling, 60일 × 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ayg4l3jf0te",
   "metadata": {},
   "source": [
    "3. Sliding Windows 생성\n",
    "- X: (n_samples, input_time, 10) — value, temperature, rainfall, humidity + 6 cyclical features\n",
    "- y: (n_samples, output_time) — value만 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9md5y74ue97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Settings ===\n",
      "Input: 72 steps (72min = 1.2h), 10 features\n",
      "Output: 15 steps (15min) × 4 rolling = 60min\n"
     ]
    }
   ],
   "source": [
    "feature_cols = ['value', 'temperature', 'rainfall', 'humidity',\n",
    "                'time_sin', 'time_cos', 'dow_sin', 'dow_cos',\n",
    "                'season_sin', 'season_cos']\n",
    "scale_cols = ['value', 'temperature', 'rainfall', 'humidity']  # MinMaxScaler target (sin/cos excluded)\n",
    "target_col = 'value'\n",
    "input_time = 72    # 72 steps × 1min = 72min\n",
    "output_time = 15   # 15 steps × 1min = 15min (× 4 rolling = 1h)\n",
    "\n",
    "print(f\"=== Settings ===\")\n",
    "print(f\"Input: {input_time} steps ({input_time}min = {input_time/60:.1f}h), {len(feature_cols)} features\")\n",
    "print(f\"Output: {output_time} steps ({output_time}min) × 4 rolling = {output_time*4}min\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9g03ebo0hw",
   "metadata": {},
   "source": [
    "4. Train / Val / Test Split (0.7 / 0.15 / 0.15)\n",
    "- Split 경계에 gap 적용 → sliding window 겹침(data leak) 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cboyx24gome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0: 41,514 samples (train=29,059 / val=6,227 / test=6,054) gap=87\n",
      "Block 1: 55,304 samples (train=38,712 / val=8,295 / test=8,123) gap=87\n",
      "Block 2: 76,315 samples (train=53,420 / val=11,447 / test=11,274) gap=87\n",
      "Block 3: 67,289 samples (train=47,102 / val=10,093 / test=9,920) gap=87\n",
      "\n",
      "=== Sliding Window + Split (Seasonal Blocks × Segments, gap=87) ===\n",
      "★ Segment-aware: 603개 세그먼트 사용, 1557개 스킵 (< 87 steps)\n",
      "X shape: (72, 10), y shape: (15,)\n",
      "Train: 168,293 | Val: 36,062 | Test: 35,371 | Total: 239,726\n",
      "Memory: X_train 484.7 MB, y_train 10.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Sliding window 생성 + Train/Val/Test split (block × segment 독립)\n",
    "# ★ 수정: segment_id 경계를 넘는 window 생성 방지\n",
    "# split 경계에 gap 적용 → sliding window 겹침(data leak) 방지\n",
    "\n",
    "train_ratio, val_ratio = 0.7, 0.15\n",
    "split_gap = input_time + output_time  # 87 steps gap (data leak 방지)\n",
    "min_segment_len = input_time + output_time  # 세그먼트 최소 길이\n",
    "\n",
    "X_train_list, y_train_list = [], []\n",
    "X_val_list, y_val_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "test_times_list = []\n",
    "\n",
    "skipped_segments = 0\n",
    "used_segments = 0\n",
    "\n",
    "for block_id in sorted(df_merged['block_id'].unique()):\n",
    "    block = df_merged[df_merged['block_id'] == block_id]\n",
    "    \n",
    "    # ★ block 내 segment별로 sliding window 생성\n",
    "    seg_X, seg_y, seg_times = [], [], []\n",
    "    \n",
    "    for seg_id in sorted(block['segment_id'].unique()):\n",
    "        segment = block[block['segment_id'] == seg_id].reset_index(drop=True)\n",
    "        \n",
    "        if len(segment) < min_segment_len:\n",
    "            skipped_segments += 1\n",
    "            continue  # 너무 짧은 세그먼트 스킵\n",
    "        \n",
    "        used_segments += 1\n",
    "        seg_features = segment[feature_cols].values.astype(np.float32)\n",
    "        seg_target = segment[target_col].values.astype(np.float32)\n",
    "        seg_time = segment['time'].values\n",
    "        \n",
    "        n_samples = len(segment) - input_time - output_time + 1\n",
    "        for i in range(n_samples):\n",
    "            seg_X.append(seg_features[i : i + input_time])\n",
    "            seg_y.append(seg_target[i + input_time : i + input_time + output_time])\n",
    "            seg_times.append(seg_time[i + input_time])\n",
    "    \n",
    "    if len(seg_X) == 0:\n",
    "        print(f\"Block {block_id}: 유효 윈도우 없음 (스킵)\")\n",
    "        continue\n",
    "    \n",
    "    X_block = np.array(seg_X)\n",
    "    y_block = np.array(seg_y)\n",
    "    times_block = np.array(seg_times)\n",
    "    n_samples = len(X_block)\n",
    "\n",
    "    # gap을 두어 Train/Val/Test 간 window 겹침 방지\n",
    "    t_end = int(n_samples * train_ratio)\n",
    "    v_start = t_end + split_gap\n",
    "    v_end = v_start + int(n_samples * val_ratio)\n",
    "    test_start = v_end + split_gap\n",
    "\n",
    "    X_train_list.append(X_block[:t_end])\n",
    "    y_train_list.append(y_block[:t_end])\n",
    "    X_val_list.append(X_block[v_start:v_end])\n",
    "    y_val_list.append(y_block[v_start:v_end])\n",
    "    X_test_list.append(X_block[test_start:])\n",
    "    y_test_list.append(y_block[test_start:])\n",
    "\n",
    "    for i in range(test_start, n_samples):\n",
    "        test_times_list.append(times_block[i])\n",
    "\n",
    "    n_train = t_end\n",
    "    n_val = v_end - v_start\n",
    "    n_test = n_samples - test_start\n",
    "    print(f\"Block {block_id}: {n_samples:,} samples \"\n",
    "          f\"(train={n_train:,} / val={n_val:,} / test={n_test:,}) \"\n",
    "          f\"gap={split_gap}\")\n",
    "\n",
    "X_train = np.concatenate(X_train_list)\n",
    "y_train = np.concatenate(y_train_list)\n",
    "X_val = np.concatenate(X_val_list)\n",
    "y_val = np.concatenate(y_val_list)\n",
    "X_test = np.concatenate(X_test_list)\n",
    "y_test = np.concatenate(y_test_list)\n",
    "test_times = np.array(test_times_list)\n",
    "\n",
    "n_total = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"\\n=== Sliding Window + Split (Seasonal Blocks × Segments, gap={split_gap}) ===\")\n",
    "print(f\"★ Segment-aware: {used_segments}개 세그먼트 사용, {skipped_segments}개 스킵 (< {min_segment_len} steps)\")\n",
    "print(f\"X shape: ({input_time}, {len(feature_cols)}), y shape: ({output_time},)\")\n",
    "print(f\"Train: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,} | Total: {n_total:,}\")\n",
    "print(f\"Memory: X_train {X_train.nbytes/1e6:.1f} MB, y_train {y_train.nbytes/1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7qtwynq7uec",
   "metadata": {},
   "source": [
    "5. 정규화 (Train 기준)\n",
    "- X: feature별 개별 MinMaxScaler (value, temperature, rainfall, humidity)\n",
    "- y: value scaler로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de9h20c77vf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 기준 Scaler 범위 (scale_cols만):\n",
      "         value: [0.00, 310.26]\n",
      "   temperature: [-11.70, 37.70]\n",
      "      rainfall: [0.00, 55.50]\n",
      "      humidity: [4.80, 99.90]\n",
      "\n",
      "sin/cos features (6개): 정규화 없이 원본 유지 [0, 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "n_features = len(feature_cols)\n",
    "\n",
    "# Train 데이터로만 scaler fit (scale_cols만)\n",
    "scalers = {}\n",
    "for col in scale_cols:\n",
    "    i = feature_cols.index(col)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train[:, :, i].reshape(-1, 1))\n",
    "    scalers[col] = scaler\n",
    "\n",
    "# X 정규화 (scale_cols만, sin/cos 제외)\n",
    "def normalize_X(arr):\n",
    "    arr = arr.copy()\n",
    "    for col in scale_cols:\n",
    "        i = feature_cols.index(col)\n",
    "        s = scalers[col]\n",
    "        d_min, d_max = np.float32(s.data_min_[0]), np.float32(s.data_max_[0])\n",
    "        arr[:, :, i] = (arr[:, :, i] - d_min) / (d_max - d_min)\n",
    "    return arr\n",
    "\n",
    "X_train_scaled = normalize_X(X_train)\n",
    "X_val_scaled = normalize_X(X_val)\n",
    "X_test_scaled = normalize_X(X_test)\n",
    "\n",
    "# y 정규화 (value scaler 사용)\n",
    "val_min = np.float32(scalers['value'].data_min_[0])\n",
    "val_max = np.float32(scalers['value'].data_max_[0])\n",
    "\n",
    "def normalize_y(arr):\n",
    "    return (arr - val_min) / (val_max - val_min)\n",
    "\n",
    "def denormalize_y(arr):\n",
    "    return arr * (val_max - val_min) + val_min\n",
    "\n",
    "y_train_scaled = normalize_y(y_train)\n",
    "y_val_scaled = normalize_y(y_val)\n",
    "y_test_scaled = normalize_y(y_test)\n",
    "\n",
    "print(f\"Train 기준 Scaler 범위 (scale_cols만):\")\n",
    "for col in scale_cols:\n",
    "    s = scalers[col]\n",
    "    print(f\"  {col:>12s}: [{s.data_min_[0]:.2f}, {s.data_max_[0]:.2f}]\")\n",
    "print(f\"\\nsin/cos features ({n_features - len(scale_cols)}개): 정규화 없이 원본 유지 [0, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensor 변환 & DataLoader (pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Train: 657 batches | Val: 71 | Test: 70\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train_scaled)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val_scaled)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test_scaled)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 256\n",
    "use_pin = (device.type == 'cuda')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          drop_last=True, pin_memory=use_pin, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False,\n",
    "                        pin_memory=use_pin, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False,\n",
    "                         pin_memory=use_pin, num_workers=0)\n",
    "\n",
    "print(f\"Train: {len(train_loader)} batches | Val: {len(val_loader)} | Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model (A: dropout=0.2 (v2 유지), LN순서=Drop→LN→FC (v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMSeq2SeqAttnModel(nn.Module):\n",
    "    \"\"\"v2: Decoder dropout=0.2, Drop→LN→FC 순서\"\"\"\n",
    "    def __init__(self, input_size=10, hidden_size=128, num_layers=2,\n",
    "                 output_size=15, embed_dim=16, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_size, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.step_embedding = nn.Embedding(output_size, embed_dim)\n",
    "\n",
    "        self.attn_Wh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.attn_Ws = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.attn_V = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "        self.decoder = nn.LSTMCell(\n",
    "            input_size=hidden_size + embed_dim, hidden_size=hidden_size\n",
    "        )\n",
    "        # v2: Dropout → LayerNorm → FC\n",
    "        self.dec_dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        enc_outputs, (h_n, c_n) = self.encoder(x)\n",
    "        enc_keys_projected = self.attn_Ws(enc_outputs)\n",
    "\n",
    "        h_dec = h_n[-1]\n",
    "        c_dec = c_n[-1]\n",
    "\n",
    "        predictions = []\n",
    "        self.attn_weights_all = []\n",
    "        step_ids = torch.arange(self.output_size, device=x.device)\n",
    "        step_embs = self.step_embedding(step_ids)\n",
    "\n",
    "        for t in range(self.output_size):\n",
    "            query = self.attn_Wh(h_dec).unsqueeze(1)\n",
    "            energy = torch.tanh(query + enc_keys_projected)\n",
    "            score = self.attn_V(energy).squeeze(-1)\n",
    "            attn_weights = torch.softmax(score, dim=1)\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), enc_outputs).squeeze(1)\n",
    "            self.attn_weights_all.append(attn_weights.detach())\n",
    "\n",
    "            step_emb = step_embs[t].unsqueeze(0).expand(batch_size, -1)\n",
    "            dec_input = torch.cat([context, step_emb], dim=1)\n",
    "            h_dec, c_dec = self.decoder(dec_input, (h_dec, c_dec))\n",
    "\n",
    "            # v2 순서: Dropout → LayerNorm → FC\n",
    "            h_out = self.dec_dropout(h_dec)\n",
    "            h_out = self.layer_norm(h_out)\n",
    "            pred_t = self.fc_out(h_out)\n",
    "            predictions.append(pred_t)\n",
    "\n",
    "        predictions = torch.cat(predictions, dim=1)\n",
    "        self.attn_weights_all = torch.stack(self.attn_weights_all, dim=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 377,585\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Ablation_A\"\n",
    "model_check = LSTMSeq2SeqAttnModel(\n",
    "    input_size=n_features, hidden_size=128, num_layers=2,\n",
    "    output_size=output_time, embed_dim=16, dropout=0.2,\n",
    ")\n",
    "total_p = sum(p.numel() for p in model_check.parameters())\n",
    "print(f\"Parameters: {total_p:,}\")\n",
    "del model_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=1e-5, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss: float | None = None\n",
    "        self.early_stop = False\n",
    "        self.best_model: dict[str, torch.Tensor] | None = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "            \n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f}). Saving model...')\n",
    "        self.best_model = model.state_dict().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 학습 (Multi-Run, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "es_patience = 10\n",
    "\n",
    "step_weights = torch.linspace(1.0, 2.0, output_time).to(device)\n",
    "\n",
    "def weighted_mse_loss(pred, target):\n",
    "    return (step_weights * (pred - target) ** 2).mean()\n",
    "\n",
    "criterion = weighted_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Run 1/3 (seed=42)\n",
      "============================================================\n",
      "  Epoch [ 10/100] Train: 0.001518 Val: 0.002994 LR: 0.001000\n",
      "  Epoch [ 20/100] Train: 0.001220 Val: 0.002109 LR: 0.001000\n",
      "  Early Stopping at Epoch 28 (Best Val Loss: 0.001377)\n",
      "  → MAPE: 5.85% | SMAPE: 5.97% | R²: 0.9673 | Bias: -1.26 | Epoch: 28\n",
      "\n",
      "============================================================\n",
      "Run 2/3 (seed=123)\n",
      "============================================================\n",
      "  Epoch [ 10/100] Train: 0.001638 Val: 0.002680 LR: 0.001000\n",
      "  Epoch [ 20/100] Train: 0.001327 Val: 0.001864 LR: 0.000500\n",
      "  Epoch [ 30/100] Train: 0.001088 Val: 0.001422 LR: 0.000250\n",
      "  Epoch [ 40/100] Train: 0.000993 Val: 0.001460 LR: 0.000063\n",
      "  Early Stopping at Epoch 40 (Best Val Loss: 0.001422)\n",
      "  → MAPE: 5.91% | SMAPE: 6.16% | R²: 0.9725 | Bias: +1.44 | Epoch: 40\n",
      "\n",
      "============================================================\n",
      "Run 3/3 (seed=7)\n",
      "============================================================\n",
      "  Epoch [ 10/100] Train: 0.001820 Val: 0.002894 LR: 0.001000\n",
      "  Epoch [ 20/100] Train: 0.001182 Val: 0.001386 LR: 0.000500\n",
      "  Epoch [ 30/100] Train: 0.000962 Val: 0.001391 LR: 0.000125\n",
      "  Early Stopping at Epoch 34 (Best Val Loss: 0.001334)\n",
      "  → MAPE: 6.02% | SMAPE: 6.26% | R²: 0.9716 | Bias: +1.07 | Epoch: 34\n",
      "\n",
      "============================================================\n",
      "All runs completed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def calc_mape(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calc_smape(y_true, y_pred):\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    mask = denom > 0\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / denom[mask]) * 100\n",
    "\n",
    "all_run_results = []\n",
    "all_run_models = []\n",
    "all_run_losses = []\n",
    "\n",
    "for run_idx, seed in enumerate(SEEDS[:N_RUNS]):\n",
    "    set_seed(seed)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Run {run_idx+1}/{N_RUNS} (seed={seed})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = LSTMSeq2SeqAttnModel(\n",
    "        input_size=n_features, hidden_size=128, num_layers=2,\n",
    "        output_size=output_time, embed_dim=16, dropout=0.2,\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3\n",
    "    )\n",
    "    early_stopping = EarlyStopping(patience=es_patience, verbose=False)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss_epoch = 0.0\n",
    "        for batch_X, targets in train_loader:\n",
    "            batch_X, targets = batch_X.to(device), targets.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss_epoch += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss_epoch = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, targets in val_loader:\n",
    "                batch_X, targets = batch_X.to(device), targets.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss_epoch += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss_epoch / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'  Epoch [{epoch+1:3d}/{num_epochs}] '\n",
    "                  f'Train: {avg_train_loss:.6f} '\n",
    "                  f'Val: {avg_val_loss:.6f} '\n",
    "                  f'LR: {current_lr:.6f}')\n",
    "        \n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"  Early Stopping at Epoch {epoch+1} \"\n",
    "                  f\"(Best Val Loss: {early_stopping.best_loss:.6f})\")\n",
    "            break\n",
    "    \n",
    "    if early_stopping.best_model is not None:\n",
    "        model.load_state_dict(early_stopping.best_model)\n",
    "    \n",
    "    all_run_models.append(copy.deepcopy(model.state_dict()))\n",
    "    all_run_losses.append({'train': train_losses, 'val': val_losses})\n",
    "    \n",
    "    model.eval()\n",
    "    test_preds_list, test_actuals_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            test_preds_list.append(outputs.cpu().numpy())\n",
    "            test_actuals_list.append(batch_y.numpy())\n",
    "    \n",
    "    run_preds = denormalize_y(np.vstack(test_preds_list))\n",
    "    run_actuals = denormalize_y(np.vstack(test_actuals_list))\n",
    "    \n",
    "    run_result = {\n",
    "        'seed': seed,\n",
    "        'epoch': len(train_losses),\n",
    "        'mape': calc_mape(run_actuals.flatten(), run_preds.flatten()),\n",
    "        'smape': calc_smape(run_actuals.flatten(), run_preds.flatten()),\n",
    "        'rmse': np.sqrt(mean_squared_error(run_actuals, run_preds)),\n",
    "        'mae': mean_absolute_error(run_actuals, run_preds),\n",
    "        'r2': r2_score(run_actuals.flatten(), run_preds.flatten()),\n",
    "        'bias': float(np.mean(run_actuals - run_preds)),\n",
    "        'predictions': run_preds,\n",
    "        'actuals': run_actuals,\n",
    "    }\n",
    "    \n",
    "    step_mapes = []\n",
    "    for s in range(output_time):\n",
    "        step_mapes.append(calc_mape(run_actuals[:, s], run_preds[:, s]))\n",
    "    run_result['step_mapes'] = step_mapes\n",
    "    \n",
    "    season_mapes = []\n",
    "    offset = 0\n",
    "    for size in [len(arr) for arr in X_test_list]:\n",
    "        block_a = run_actuals[offset:offset+size]\n",
    "        block_p = run_preds[offset:offset+size]\n",
    "        season_mapes.append(calc_mape(block_a.flatten(), block_p.flatten()))\n",
    "        offset += size\n",
    "    run_result['season_mapes'] = season_mapes\n",
    "    run_result['macro_mape'] = np.mean(season_mapes)\n",
    "    \n",
    "    all_run_results.append(run_result)\n",
    "    \n",
    "    print(f\"  → MAPE: {run_result['mape']:.2f}% | SMAPE: {run_result['smape']:.2f}% \"\n",
    "          f\"| R²: {run_result['r2']:.4f} | Bias: {run_result['bias']:+.2f} \"\n",
    "          f\"| Epoch: {run_result['epoch']}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All runs completed\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A 결과 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Multi-Run Summary (3 runs)\n",
      "======================================================================\n",
      "\n",
      "Metric                Mean       Std       Min       Max\n",
      "-------------------------------------------------------\n",
      "MAPE (%)            5.9236    0.0717    5.8453    6.0185\n",
      "SMAPE (%)           6.1301    0.1170    5.9747    6.2572\n",
      "RMSE                9.7197    0.3665    9.3905   10.2310\n",
      "MAE                 7.6494    0.2662    7.3586    8.0018\n",
      "R²                  0.9705    0.0023    0.9673    0.9725\n",
      "Bias                0.4182    1.1990   -1.2639    1.4447\n",
      "Macro MAPE (%)      5.8722    0.0634    5.8090    5.9589\n",
      "\n",
      "각 Run 상세:\n",
      "  seed= 42: MAPE=5.85% SMAPE=5.97% R²=0.9673 Bias=-1.26 Epoch=28\n",
      "  seed=123: MAPE=5.91% SMAPE=6.16% R²=0.9725 Bias=+1.44 Epoch=40\n",
      "  seed=  7: MAPE=6.02% SMAPE=6.26% R²=0.9716 Bias=+1.07 Epoch=34\n",
      "\n",
      "Step별 MAPE (mean ± std):\n",
      "  Step  1: 4.57% ± 0.26%\n",
      "  Step  2: 4.87% ± 0.28%\n",
      "  Step  3: 5.06% ± 0.18%\n",
      "  Step  4: 4.99% ± 0.19%\n",
      "  Step  5: 5.06% ± 0.15%\n",
      "  Step  6: 5.21% ± 0.10%\n",
      "  Step  7: 5.41% ± 0.05%\n",
      "  Step  8: 5.65% ± 0.01%\n",
      "  Step  9: 5.92% ± 0.04%\n",
      "  Step 10: 6.21% ± 0.06%\n",
      "  Step 11: 6.52% ± 0.07%\n",
      "  Step 12: 6.84% ± 0.07%\n",
      "  Step 13: 7.17% ± 0.05%\n",
      "  Step 14: 7.51% ± 0.04%\n",
      "  Step 15: 7.85% ± 0.07%\n",
      "\n",
      "계절별 MAPE (mean ± std):\n",
      "  Winter  : 5.18% ± 0.17%\n",
      "  Spring  : 6.46% ± 0.30%\n",
      "  Summer  : 6.02% ± 0.08%\n",
      "  Fall    : 5.82% ± 0.05%\n",
      "\n",
      "★ Best Run: seed=42 (MAPE=5.85%)\n"
     ]
    }
   ],
   "source": [
    "metric_labels = {\n",
    "    'mape': 'MAPE (%)', 'smape': 'SMAPE (%)', 'rmse': 'RMSE',\n",
    "    'mae': 'MAE', 'r2': 'R²', 'bias': 'Bias', 'macro_mape': 'Macro MAPE (%)'\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Multi-Run Summary ({N_RUNS} runs)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "metrics = ['mape', 'smape', 'rmse', 'mae', 'r2', 'bias', 'macro_mape']\n",
    "print(f\"\\n{'Metric':<16s}  {'Mean':>8s}  {'Std':>8s}  {'Min':>8s}  {'Max':>8s}\")\n",
    "print(\"-\" * 55)\n",
    "for m in metrics:\n",
    "    vals = [r[m] for r in all_run_results]\n",
    "    print(f\"{metric_labels[m]:<16s}  {np.mean(vals):8.4f}  {np.std(vals):8.4f}  \"\n",
    "          f\"{np.min(vals):8.4f}  {np.max(vals):8.4f}\")\n",
    "\n",
    "print(f\"\\n각 Run 상세:\")\n",
    "for r in all_run_results:\n",
    "    print(f\"  seed={r['seed']:3d}: MAPE={r['mape']:.2f}% SMAPE={r['smape']:.2f}% \"\n",
    "          f\"R²={r['r2']:.4f} Bias={r['bias']:+.2f} Epoch={r['epoch']}\")\n",
    "\n",
    "print(f\"\\nStep별 MAPE (mean ± std):\")\n",
    "for s in range(output_time):\n",
    "    vals = [r['step_mapes'][s] for r in all_run_results]\n",
    "    print(f\"  Step {s+1:2d}: {np.mean(vals):.2f}% ± {np.std(vals):.2f}%\")\n",
    "\n",
    "season_names_list = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "print(f\"\\n계절별 MAPE (mean ± std):\")\n",
    "for i, name in enumerate(season_names_list):\n",
    "    vals = [r['season_mapes'][i] for r in all_run_results]\n",
    "    print(f\"  {name:8s}: {np.mean(vals):.2f}% ± {np.std(vals):.2f}%\")\n",
    "\n",
    "best_idx = np.argmin([r['mape'] for r in all_run_results])\n",
    "best_run = all_run_results[best_idx]\n",
    "print(f\"\\n★ Best Run: seed={best_run['seed']} (MAPE={best_run['mape']:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "314env (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
