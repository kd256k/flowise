{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. flow 전처리 (IQR, Savgol)\n",
    "1. weather 전처리 \n",
    "2. flow & weather merge\n",
    "3. sliding window \n",
    "4. Train / Val / Test Split (0.7 / 0.15 / 0.15)\n",
    "5. 정규화 (Train기준)\n",
    "6. 모델 생성 LSTMSeq2SeqAttnModel \n",
    "7~8. 학습 (patience=10, N=3) → 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "268bbea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-run: 3 runs, seeds=[42, 123, 7]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 라이브러리 임포트 & 재현성 시드 설정\n",
    "# ============================================================\n",
    "# - N_RUNS: 동일 모델을 서로 다른 시드로 N회 반복 학습 (안정성 검증)\n",
    "# - set_seed(): 모든 난수 생성기(Python, NumPy, PyTorch, CUDA)를\n",
    "#   동일 시드로 고정하여 실험 재현성 보장\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, os, copy\n",
    "\n",
    "N_RUNS = 3\n",
    "SEEDS = [42, 123, 7]\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)                          # Python 기본 난수\n",
    "    np.random.seed(seed)                       # NumPy 난수\n",
    "    import torch\n",
    "    torch.manual_seed(seed)                    # PyTorch CPU 난수\n",
    "    torch.cuda.manual_seed_all(seed)           # PyTorch GPU 난수 (멀티 GPU 포함)\n",
    "    torch.backends.cudnn.deterministic = True  # cuDNN 결정론적 연산 강제\n",
    "    torch.backends.cudnn.benchmark = False     # 자동 최적화 비활성화 (재현성 우선)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)   # Python hash 시드 고정\n",
    "\n",
    "print(f\"Multi-run: {N_RUNS} runs, seeds={SEEDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mpiqw98v9mq",
   "metadata": {},
   "source": [
    "### Data Split 구조 (4계절 Block Sampling, 60일 x 4)\n",
    "\n",
    "각 블록(60일)을 시간 순서대로 70 / 15 / 15로 분할 (블록 간 87분 gap으로 data leak 방지)\n",
    "\n",
    "| 계절 | Train (~42일) | Val (~9일) | Test (~9일) |\n",
    "|------|--------------|-------------|--------------|\n",
    "| Winter | 23/12/01 ~ 24/01/11 | 24/01/11 ~ 24/01/20 | 24/01/20 ~ 24/01/29 |\n",
    "| Spring | 24/03/15 ~ 24/04/25 | 24/04/25 ~ 24/05/04 | 24/05/04 ~ 24/05/13 |\n",
    "| Summer | 23/07/01 ~ 23/08/11 | 23/08/11 ~ 23/08/20 | 23/08/20 ~ 23/08/29 |\n",
    "| Fall | 23/09/15 ~ 23/10/26 | 23/10/26 ~ 23/11/04 | 23/11/04 ~ 23/11/13 |\n",
    "\n",
    "**예측 구조:** 72분 입력 (10 features) -> 15분 예측 (flow value) x 4 rolling = 1시간\n",
    "\n",
    "**Loss:** Step-weighted MSE (step 1=1.0 -> step 15=2.0)\n",
    "**LR Scheduler:** ReduceLROnPlateau (factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef375de",
   "metadata": {},
   "source": [
    "## 0. flow, weather 데이터 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e49d44e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:28.442134Z",
     "iopub.status.busy": "2026-02-09T08:31:28.441903Z",
     "iopub.status.idle": "2026-02-09T08:31:29.231872Z",
     "shell.execute_reply": "2026-02-09T08:31:29.230864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow 원본: 943,434개 (2023-01-01 00:01:00 ~ 2024-10-17 17:19:00)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# [데이터 로드] Flow 유량 데이터 + Weather 파일 경로 설정\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = Path.cwd().parent\n",
    "\n",
    "# Flow raw data 로드 (J배수지, reservoir/10.csv)\n",
    "flow_file = BASE_DIR / \"data\" / \"rawdata\" / \"reservoir\" / \"10.csv\"\n",
    "df_flow = pd.read_csv(\n",
    "    flow_file,\n",
    "    header=None,\n",
    "    usecols=[1, 2],\n",
    "    names=['time', 'value']\n",
    ").sort_values('time').reset_index(drop=True)\n",
    "\n",
    "df_flow['time'] = pd.to_datetime(df_flow['time'], format='mixed', errors='coerce')\n",
    "df_flow = df_flow.dropna(subset=['time'])\n",
    "print(f\"Flow 원본: {len(df_flow):,}개 ({df_flow['time'].min()} ~ {df_flow['time'].max()})\")\n",
    "\n",
    "# Weather 파일 경로\n",
    "weather_file = BASE_DIR / \"data\" / \"rawdata\" / \"weather\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fmpjlrux5n",
   "metadata": {},
   "source": [
    "## 0. Flow 전처리\n",
    "- IQR 이상치 + 음수 + 급변동 제거 → NaN\n",
    "- Linear interpolation (양방향)\n",
    "- Savitzky-Golay filter (window=51, polyorder=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "omjayxm4sj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이상치 제거: IQR=21, 음수=0, 급변동(>91.06)=944, 총=962\n",
      "전처리 완료: 943,434개, 범위 [0.00, 325.52]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# [Flow 전처리] 이상치 제거 → 보간 → 스무딩\n",
    "# ============================================================\n",
    "# 1단계: IQR 이상치 + 음수값 + 급변동(상위 0.1%) → NaN 마스킹\n",
    "# 2단계: 양방향 선형 보간(interpolate) + 음수 클리핑\n",
    "# 3단계: Savitzky-Golay 필터로 고주파 노이즈 제거\n",
    "#        (window=51분, 2차 다항식 → 추세 보존 + 노이즈 억제)\n",
    "# ============================================================\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# --- 1단계: 이상치 탐지 & NaN 마스킹 ---\n",
    "Q1 = df_flow['value'].quantile(0.25)\n",
    "Q3 = df_flow['value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "iqr_mask = (df_flow['value'] < Q1 - 1.5 * IQR) | (df_flow['value'] > Q3 + 1.5 * IQR)  # IQR 범위 밖\n",
    "negative_mask = df_flow['value'] < 0                          # 음수 (물리적 불가)\n",
    "diff = df_flow['value'].diff().abs()\n",
    "spike_threshold = diff.quantile(0.999)\n",
    "spike_mask = diff > spike_threshold                            # 급변동 (상위 0.1%)\n",
    "\n",
    "total_mask = iqr_mask | negative_mask | spike_mask             # 세 조건 OR → NaN 처리\n",
    "df_flow.loc[total_mask, 'value'] = np.nan\n",
    "\n",
    "print(f\"이상치 제거: IQR={iqr_mask.sum()}, 음수={negative_mask.sum()}, \"\n",
    "      f\"급변동(>{spike_threshold:.2f})={spike_mask.sum()}, 총={total_mask.sum()}\")\n",
    "\n",
    "# --- 2단계: 선형 보간 + 음수 클리핑 ---\n",
    "df_flow['value'] = df_flow['value'].interpolate(method='linear', limit_direction='both')\n",
    "df_flow['value'] = df_flow['value'].clip(lower=0)  # 보간 결과 음수 방지\n",
    "\n",
    "# --- 3단계: Savitzky-Golay 스무딩 필터 ---\n",
    "df_flow['value'] = savgol_filter(df_flow['value'], window_length=51, polyorder=2)\n",
    "df_flow['value'] = df_flow['value'].clip(lower=0)  # 필터 결과 음수 방지\n",
    "\n",
    "print(f\"전처리 완료: {len(df_flow):,}개, \"\n",
    "      f\"범위 [{df_flow['value'].min():.2f}, {df_flow['value'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "n9muuusx0ga",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: c:\\Users\\user\\Documents\\Main\\wsl_folder\\work\\pro\\data\\processed\\flow_preprocessed.csv (943,434행)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# [저장] 전처리 완료된 Flow 데이터를 processed/ 폴더에 CSV로 저장\n",
    "# ============================================================\n",
    "# - 원본(data/rawdata/)은 수정하지 않고, 전처리 결과만 별도 저장\n",
    "# - 이후 재실행 시 이 파일을 로드하면 전처리 단계 스킵 가능\n",
    "# ============================================================\n",
    "\n",
    "save_path = BASE_DIR / \"data\" / \"processed\" / \"flow_preprocessed.csv\"\n",
    "df_flow.to_csv(save_path, index=False)\n",
    "print(f\"저장 완료: {save_path} ({len(df_flow):,}행)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42037477",
   "metadata": {},
   "source": [
    "- weather 데이터 인코딩, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6eac53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:29.309590Z",
     "iopub.status.busy": "2026-02-09T08:31:29.309370Z",
     "iopub.status.idle": "2026-02-09T08:31:29.857621Z",
     "shell.execute_reply": "2026-02-09T08:31:29.856158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "읽을 파일 수: 22\n",
      "concat 후 행 수: 799,438\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# [Weather 로드] 기상 CSV 파일 다중 인코딩 시도 후 concat\n",
    "# ============================================================\n",
    "# - 기상청 CSV는 인코딩이 파일마다 다를 수 있음 (euc-kr/cp949/utf-8)\n",
    "# - read_weather_csv(): 4가지 인코딩을 순차 시도하여 자동 감지\n",
    "# - weather 폴더 내 SYNOP CSV만 시간순 정렬 → 하나의 DataFrame으로 병합\n",
    "#   (weather_total_raw.csv 등 파생 파일은 제외)\n",
    "# ============================================================\n",
    "\n",
    "def read_weather_csv(f):\n",
    "    \"\"\"다중 인코딩 시도로 기상 CSV 1개 파일을 읽는다.\"\"\"\n",
    "    for enc in [\"euc-kr\", \"cp949\", \"utf-8\", \"utf-8-sig\"]:\n",
    "        try:\n",
    "            return pd.read_csv(f, encoding=enc)\n",
    "        except (UnicodeDecodeError, UnicodeError):\n",
    "            continue\n",
    "    raise ValueError(f\"인코딩 실패: {f.name}\")\n",
    "\n",
    "# SYNOP 원본 파일만 선택 (weather_total_raw.csv 등 파생 파일 제외)\n",
    "files = sorted([f for f in weather_file.glob(\"*.csv\") if f.name.startswith(\"SYNOP\")])\n",
    "print(f\"읽을 파일 수: {len(files)}\")\n",
    "\n",
    "weather = pd.concat(                              # 전체 파일을 하나로 병합\n",
    "    [read_weather_csv(f) for f in files],\n",
    "    ignore_index=True\n",
    ")\n",
    "print(f\"concat 후 행 수: {len(weather):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0dd991",
   "metadata": {},
   "source": [
    "- 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dbc7f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:29.859881Z",
     "iopub.status.busy": "2026-02-09T08:31:29.859672Z",
     "iopub.status.idle": "2026-02-09T08:31:31.569575Z",
     "shell.execute_reply": "2026-02-09T08:31:31.568243Z"
    }
   },
   "outputs": [],
   "source": [
    "# [저장] Weather 원본 concat 결과를 processed/에 보관 (원본 보호)\n",
    "weather.to_csv(BASE_DIR / \"data\" / \"processed\" / \"weather_total_raw.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff270d",
   "metadata": {},
   "source": [
    "## 1. weather 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35706869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime 파싱: 799,435행 성공, 3행 제거\n",
      "Rainfall 변환: 누적 → 차분\n",
      "  원본 범위: [0.00, 160.00]\n",
      "  차분 범위: [0.0000, 119.0000]\n",
      "  비영점 비율: 0.4%\n",
      "장기 결측 제거: 799,435 -> 762,326 (95.4% 유지)\n",
      "연속 세그먼트 수: 3284\n",
      "(762326, 5)\n",
      "             datetime  temperature  rainfall  humidity  segment_id\n",
      "0 2023-01-01 00:01:00         -3.3       0.0      91.6           0\n",
      "1 2023-01-01 00:02:00         -3.3       0.0      91.6           0\n",
      "2 2023-01-01 00:03:00         -3.3       0.0      91.6           0\n",
      "3 2023-01-01 00:04:00         -3.2       0.0      91.6           0\n",
      "4 2023-01-01 00:05:00         -3.2       0.0      91.6           0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# [Weather 전처리] 결측 보간, Rainfall 차분 변환, 세그먼트 ID 부여\n",
    "# ============================================================\n",
    "\n",
    "# --- 1) datetime 변환 (format='mixed'로 다양한 형식 허용, pandas 3.0 호환) ---\n",
    "weather['일시'] = pd.to_datetime(weather['일시'], format='mixed', errors='coerce')\n",
    "before_dt = len(weather)\n",
    "weather = weather.dropna(subset=['일시'])\n",
    "dropped = before_dt - len(weather)\n",
    "print(f\"datetime 파싱: {len(weather):,}행 성공\" + (f\", {dropped}행 제거\" if dropped else \"\"))\n",
    "\n",
    "# --- 2) 결측 처리 ---\n",
    "weather['0.5mm 일 누적 강수량(mm)'] = weather['0.5mm 일 누적 강수량(mm)'].fillna(0)\n",
    "weather['기온(℃)'] = weather['기온(℃)'].interpolate(method='linear', limit=60)\n",
    "weather['상대습도(%)'] = weather['상대습도(%)'].interpolate(method='linear', limit=60)\n",
    "\n",
    "# --- 3) Rainfall 차분 변환: 일 누적 → 분당 변화량 ---\n",
    "rainfall_raw = weather['0.5mm 일 누적 강수량(mm)'].copy()\n",
    "rainfall_diff = rainfall_raw.diff().fillna(0).clip(lower=0)\n",
    "\n",
    "print(f\"Rainfall 변환: 누적 → 차분\")\n",
    "print(f\"  원본 범위: [{rainfall_raw.min():.2f}, {rainfall_raw.max():.2f}]\")\n",
    "print(f\"  차분 범위: [{rainfall_diff.min():.4f}, {rainfall_diff.max():.4f}]\")\n",
    "print(f\"  비영점 비율: {(rainfall_diff > 0).mean()*100:.1f}%\")\n",
    "\n",
    "weather['0.5mm 일 누적 강수량(mm)'] = rainfall_diff\n",
    "\n",
    "# --- 4) 장기 결측(>60분 gap) 행 제거 ---\n",
    "before = len(weather)\n",
    "weather = weather.dropna(subset=['기온(℃)', '상대습도(%)'])\n",
    "print(f'장기 결측 제거: {before:,} -> {len(weather):,} ({len(weather)/before*100:.1f}% 유지)')\n",
    "\n",
    "# --- 5) 컬럼명 영문 변환 ---\n",
    "weather = weather.rename(columns={\n",
    "    '일시': 'datetime',\n",
    "    '기온(℃)': 'temperature',\n",
    "    '0.5mm 일 누적 강수량(mm)': 'rainfall',\n",
    "    '상대습도(%)': 'humidity',\n",
    "})\n",
    "\n",
    "# 중복 timestamp 제거\n",
    "weather = weather.drop_duplicates(subset='datetime', keep='first')\n",
    "weather = weather.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# --- 6) segment_id 부여 ---\n",
    "time_diff = weather['datetime'].diff()\n",
    "seg_boundary = time_diff > pd.Timedelta(minutes=1)\n",
    "weather['segment_id'] = seg_boundary.cumsum()\n",
    "print(f'연속 세그먼트 수: {weather[\"segment_id\"].nunique()}')\n",
    "\n",
    "print(weather.shape)\n",
    "print(weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed201ab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:31.929338Z",
     "iopub.status.busy": "2026-02-09T08:31:31.929140Z",
     "iopub.status.idle": "2026-02-09T08:31:32.034507Z",
     "shell.execute_reply": "2026-02-09T08:31:32.033286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>humidity</th>\n",
       "      <th>segment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>762326</td>\n",
       "      <td>762326.000000</td>\n",
       "      <td>762326.000000</td>\n",
       "      <td>762326.000000</td>\n",
       "      <td>762326.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2023-12-03 01:55:51.533543</td>\n",
       "      <td>15.311656</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>72.047670</td>\n",
       "      <td>1438.558028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2023-01-01 00:01:00</td>\n",
       "      <td>-16.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2023-06-23 20:18:15</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.400000</td>\n",
       "      <td>436.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2023-12-07 15:16:30</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.300000</td>\n",
       "      <td>931.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2024-05-15 10:50:45</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.500000</td>\n",
       "      <td>2862.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-25 15:10:00</td>\n",
       "      <td>37.700000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>99.900000</td>\n",
       "      <td>3283.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.906573</td>\n",
       "      <td>0.165693</td>\n",
       "      <td>21.186558</td>\n",
       "      <td>1202.060302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         datetime    temperature       rainfall  \\\n",
       "count                      762326  762326.000000  762326.000000   \n",
       "mean   2023-12-03 01:55:51.533543      15.311656       0.002976   \n",
       "min           2023-01-01 00:01:00     -16.500000       0.000000   \n",
       "25%           2023-06-23 20:18:15       6.200000       0.000000   \n",
       "50%           2023-12-07 15:16:30      16.400000       0.000000   \n",
       "75%           2024-05-15 10:50:45      24.600000       0.000000   \n",
       "max           2024-10-25 15:10:00      37.700000     119.000000   \n",
       "std                           NaN      10.906573       0.165693   \n",
       "\n",
       "            humidity     segment_id  \n",
       "count  762326.000000  762326.000000  \n",
       "mean       72.047670    1438.558028  \n",
       "min         2.900000       0.000000  \n",
       "25%        57.400000     436.000000  \n",
       "50%        76.300000     931.000000  \n",
       "75%        90.500000    2862.000000  \n",
       "max        99.900000    3283.000000  \n",
       "std        21.186558    1202.060302  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [확인] Weather 기술통계 — 각 feature의 분포/범위 점검\n",
    "weather.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b56b79a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:32.036640Z",
     "iopub.status.busy": "2026-02-09T08:31:32.036417Z",
     "iopub.status.idle": "2026-02-09T08:31:32.039770Z",
     "shell.execute_reply": "2026-02-09T08:31:32.038864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime       0\n",
       "temperature    0\n",
       "rainfall       0\n",
       "humidity       0\n",
       "segment_id     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [확인] 결측값(NaN) 잔존 여부 체크 — 전처리 후 0이어야 정상\n",
    "df = pd.DataFrame(weather)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b0f6d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:32.948750Z",
     "iopub.status.busy": "2026-02-09T08:31:32.948560Z",
     "iopub.status.idle": "2026-02-09T08:31:32.956793Z",
     "shell.execute_reply": "2026-02-09T08:31:32.955473Z"
    }
   },
   "outputs": [],
   "source": [
    "# [컬럼 선택] 학습에 사용할 Weather 컬럼만 추출 (+ segment_id)\n",
    "df_weather = df[['datetime', 'temperature', 'rainfall', 'humidity', 'segment_id']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8bbb2",
   "metadata": {},
   "source": [
    "## 2. weather data merge & weather, time feature 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f7ded0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:33.072829Z",
     "iopub.status.busy": "2026-02-09T08:31:33.072617Z",
     "iopub.status.idle": "2026-02-09T08:31:33.125952Z",
     "shell.execute_reply": "2026-02-09T08:31:33.124479Z"
    }
   },
   "outputs": [],
   "source": [
    "# [Merge] Flow + Weather를 timestamp(time ↔ datetime) 기준 inner join\n",
    "# - inner join이므로 양쪽 모두 존재하는 시각만 남음\n",
    "df_merged = pd.merge(df_flow, df_weather, how='inner', left_on='time', right_on=\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0b5e38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:33.147654Z",
     "iopub.status.busy": "2026-02-09T08:31:33.147472Z",
     "iopub.status.idle": "2026-02-09T08:31:33.151697Z",
     "shell.execute_reply": "2026-02-09T08:31:33.150351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터: 750,296개 (1분 단위)\n",
      "merge 후 연속 세그먼트 수: 3789\n",
      "Temporal features 추가 완료: (750296, 12)\n",
      "                 time      value  temperature  rainfall  humidity  segment_id  \\\n",
      "0 2023-01-01 00:01:00  96.577302         -3.3       0.0      91.6           0   \n",
      "1 2023-01-01 00:02:00  96.739744         -3.3       0.0      91.6           0   \n",
      "2 2023-01-01 00:03:00  96.895855         -3.3       0.0      91.6           0   \n",
      "3 2023-01-01 00:04:00  97.045636         -3.2       0.0      91.6           0   \n",
      "4 2023-01-01 00:05:00  97.189086         -3.2       0.0      91.6           0   \n",
      "\n",
      "   time_sin  time_cos   dow_sin   dow_cos  season_sin  season_cos  \n",
      "0  0.502182  0.999995  0.109084  0.811745    0.508601    0.999926  \n",
      "1  0.504363  0.999981  0.109084  0.811745    0.508601    0.999926  \n",
      "2  0.506545  0.999957  0.109084  0.811745    0.508601    0.999926  \n",
      "3  0.508726  0.999924  0.109084  0.811745    0.508601    0.999926  \n",
      "4  0.510907  0.999881  0.109084  0.811745    0.508601    0.999926  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# [후처리] merge 후 정리 + Cyclical Temporal Features 생성\n",
    "# ============================================================\n",
    "# 1) 중복 datetime 컬럼 제거, 정렬, NaN 행 제거\n",
    "# 2) segment_id 재계산: merge로 행이 빠졌으므로 불연속 경계 재탐지\n",
    "# 3) Cyclical encoding: sin/cos 변환으로 주기성을 [0,1] 범위 연속값으로 표현\n",
    "#    - time_sin/cos : 하루 주기 (T=1440분)  → 시간대별 유량 패턴\n",
    "#    - dow_sin/cos  : 주간 주기 (T=7일)     → 평일/주말 패턴\n",
    "#    - season_sin/cos: 연간 주기 (T=365.25일) → 계절별 패턴\n",
    "#    0.5*sin()+0.5 변환으로 [0,1] 범위 유지 (MinMaxScaler 불필요)\n",
    "# ============================================================\n",
    "\n",
    "df_merged = df_merged.drop(columns=['datetime'])  # merge로 생긴 중복 컬럼 제거\n",
    "\n",
    "df_merged = df_merged.sort_values('time').reset_index(drop=True)\n",
    "df_merged = df_merged.dropna()\n",
    "print(f\"데이터: {len(df_merged):,}개 (1분 단위)\")\n",
    "\n",
    "# --- segment_id 재계산 (merge 후 실제 불연속 기준) ---\n",
    "time_diff_merged = df_merged['time'].diff()\n",
    "seg_boundary_merged = time_diff_merged > pd.Timedelta(minutes=1)\n",
    "df_merged['segment_id'] = seg_boundary_merged.cumsum()\n",
    "print(f\"merge 후 연속 세그먼트 수: {df_merged['segment_id'].nunique()}\")\n",
    "\n",
    "# --- Cyclical Temporal Features ---\n",
    "t: pd.Series = df_merged['time']\n",
    "\n",
    "hour = t.dt.hour.astype(np.float64)\n",
    "minute = t.dt.minute.astype(np.float64)\n",
    "dow = t.dt.dayofweek.astype(np.float64)\n",
    "doy = t.dt.dayofyear.astype(np.float64)\n",
    "\n",
    "# 하루 주기 (분 단위, T=1440분)\n",
    "minute_of_day = hour * 60 + minute\n",
    "df_merged['time_sin'] = 0.5 * np.sin(2 * np.pi * minute_of_day / 1440) + 0.5\n",
    "df_merged['time_cos'] = 0.5 * np.cos(2 * np.pi * minute_of_day / 1440) + 0.5\n",
    "\n",
    "# 주간 주기 (요일, T=7)\n",
    "df_merged['dow_sin'] = 0.5 * np.sin(2 * np.pi * dow / 7) + 0.5\n",
    "df_merged['dow_cos'] = 0.5 * np.cos(2 * np.pi * dow / 7) + 0.5\n",
    "\n",
    "# 연간 주기 (일수, T=365.25)\n",
    "df_merged['season_sin'] = 0.5 * np.sin(2 * np.pi * doy / 365.25) + 0.5\n",
    "df_merged['season_cos'] = 0.5 * np.cos(2 * np.pi * doy / 365.25) + 0.5\n",
    "\n",
    "print(f\"Temporal features 추가 완료: {df_merged.shape}\")\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ee3a4f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-09T08:31:33.164994Z",
     "iopub.status.busy": "2026-02-09T08:31:33.164815Z",
     "iopub.status.idle": "2026-02-09T08:31:33.295128Z",
     "shell.execute_reply": "2026-02-09T08:31:33.293831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 750296 entries, 0 to 750295\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   time         750296 non-null  datetime64[us]\n",
      " 1   value        750296 non-null  float64       \n",
      " 2   temperature  750296 non-null  float64       \n",
      " 3   rainfall     750296 non-null  float64       \n",
      " 4   humidity     750296 non-null  float64       \n",
      " 5   segment_id   750296 non-null  int64         \n",
      " 6   time_sin     750296 non-null  float64       \n",
      " 7   time_cos     750296 non-null  float64       \n",
      " 8   dow_sin      750296 non-null  float64       \n",
      " 9   dow_cos      750296 non-null  float64       \n",
      " 10  season_sin   750296 non-null  float64       \n",
      " 11  season_cos   750296 non-null  float64       \n",
      "dtypes: datetime64[us](1), float64(10), int64(1)\n",
      "memory usage: 68.7 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((750296, 12), None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [확인] merge 결과 shape & dtype 점검\n",
    "df_merged.shape, df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2569d9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>value</th>\n",
       "      <th>temperature</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>humidity</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>time_sin</th>\n",
       "      <th>time_cos</th>\n",
       "      <th>dow_sin</th>\n",
       "      <th>dow_cos</th>\n",
       "      <th>season_sin</th>\n",
       "      <th>season_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>750296</td>\n",
       "      <td>750296.000000</td>\n",
       "      <td>750296.000000</td>\n",
       "      <td>750296.000000</td>\n",
       "      <td>750296.000000</td>\n",
       "      <td>750296.000000</td>\n",
       "      <td>750296.000000</td>\n",
       "      <td>750296.000000</td>\n",
       "      <td>750296.000000</td>\n",
       "      <td>750296.000000</td>\n",
       "      <td>7.502960e+05</td>\n",
       "      <td>750296.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2023-11-28 04:26:42.255643</td>\n",
       "      <td>151.178574</td>\n",
       "      <td>15.306791</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>71.904041</td>\n",
       "      <td>1667.789278</td>\n",
       "      <td>0.503187</td>\n",
       "      <td>0.498909</td>\n",
       "      <td>0.497432</td>\n",
       "      <td>0.497254</td>\n",
       "      <td>5.241825e-01</td>\n",
       "      <td>0.470597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2023-01-01 00:01:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012536</td>\n",
       "      <td>0.049516</td>\n",
       "      <td>2.889877e-07</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2023-06-21 20:52:45</td>\n",
       "      <td>108.331885</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.300000</td>\n",
       "      <td>562.000000</td>\n",
       "      <td>0.149545</td>\n",
       "      <td>0.144907</td>\n",
       "      <td>0.109084</td>\n",
       "      <td>0.049516</td>\n",
       "      <td>1.605898e-01</td>\n",
       "      <td>0.134311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2023-12-03 15:42:30</td>\n",
       "      <td>143.890453</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.100000</td>\n",
       "      <td>1171.000000</td>\n",
       "      <td>0.508726</td>\n",
       "      <td>0.497818</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.388740</td>\n",
       "      <td>5.771020e-01</td>\n",
       "      <td>0.440471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2024-05-09 11:17:15</td>\n",
       "      <td>192.041955</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.300000</td>\n",
       "      <td>3139.000000</td>\n",
       "      <td>0.856625</td>\n",
       "      <td>0.852007</td>\n",
       "      <td>0.890916</td>\n",
       "      <td>0.811745</td>\n",
       "      <td>8.768488e-01</td>\n",
       "      <td>0.817949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-17 17:19:00</td>\n",
       "      <td>325.516638</td>\n",
       "      <td>37.700000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>99.900000</td>\n",
       "      <td>3788.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.999928e-01</td>\n",
       "      <td>0.999926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>56.916589</td>\n",
       "      <td>10.971803</td>\n",
       "      <td>0.166925</td>\n",
       "      <td>21.225753</td>\n",
       "      <td>1331.184225</td>\n",
       "      <td>0.353635</td>\n",
       "      <td>0.353456</td>\n",
       "      <td>0.354114</td>\n",
       "      <td>0.352972</td>\n",
       "      <td>3.560774e-01</td>\n",
       "      <td>0.348941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             time          value    temperature  \\\n",
       "count                      750296  750296.000000  750296.000000   \n",
       "mean   2023-11-28 04:26:42.255643     151.178574      15.306791   \n",
       "min           2023-01-01 00:01:00       0.000000     -16.500000   \n",
       "25%           2023-06-21 20:52:45     108.331885       6.100000   \n",
       "50%           2023-12-03 15:42:30     143.890453      16.400000   \n",
       "75%           2024-05-09 11:17:15     192.041955      24.700000   \n",
       "max           2024-10-17 17:19:00     325.516638      37.700000   \n",
       "std                           NaN      56.916589      10.971803   \n",
       "\n",
       "            rainfall       humidity     segment_id       time_sin  \\\n",
       "count  750296.000000  750296.000000  750296.000000  750296.000000   \n",
       "mean        0.002967      71.904041    1667.789278       0.503187   \n",
       "min         0.000000       2.900000       0.000000       0.000000   \n",
       "25%         0.000000      57.300000     562.000000       0.149545   \n",
       "50%         0.000000      76.100000    1171.000000       0.508726   \n",
       "75%         0.000000      90.300000    3139.000000       0.856625   \n",
       "max       119.000000      99.900000    3788.000000       1.000000   \n",
       "std         0.166925      21.225753    1331.184225       0.353635   \n",
       "\n",
       "            time_cos        dow_sin        dow_cos    season_sin  \\\n",
       "count  750296.000000  750296.000000  750296.000000  7.502960e+05   \n",
       "mean        0.498909       0.497432       0.497254  5.241825e-01   \n",
       "min         0.000000       0.012536       0.049516  2.889877e-07   \n",
       "25%         0.144907       0.109084       0.049516  1.605898e-01   \n",
       "50%         0.497818       0.500000       0.388740  5.771020e-01   \n",
       "75%         0.852007       0.890916       0.811745  8.768488e-01   \n",
       "max         0.999995       0.987464       1.000000  9.999928e-01   \n",
       "std         0.353456       0.354114       0.352972  3.560774e-01   \n",
       "\n",
       "          season_cos  \n",
       "count  750296.000000  \n",
       "mean        0.470597  \n",
       "min         0.000010  \n",
       "25%         0.134311  \n",
       "50%         0.440471  \n",
       "75%         0.817949  \n",
       "max         0.999926  \n",
       "std         0.348941  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [확인] merge 후 전체 기술통계 — 각 feature 분포 점검\n",
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "qi3t6a7l9i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Winter] 2023-12-01 ~ 2024-01-29: 71,779 rows\n",
      "[Spring] 2024-03-15 ~ 2024-05-13: 69,639 rows\n",
      "[Summer] 2023-07-01 ~ 2023-08-29: 70,804 rows\n",
      "[Fall] 2023-09-15 ~ 2023-11-13: 68,351 rows\n",
      "\n",
      "Total: 280,573 rows (4-season block sampling, 60일 × 4)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# [Block Sampling] 4계절 × 60일 블록 추출\n",
    "# ============================================================\n",
    "# - 전체 데이터에서 계절별 대표 구간(60일)을 선택하여 균형 잡힌 학습 데이터 구성\n",
    "# - 2023~2024년 교차 배치로 연도 편향 방지\n",
    "# - block_id: 각 계절 블록의 식별자 (0=겨울, 1=봄, 2=여름, 3=가을)\n",
    "# ============================================================\n",
    "\n",
    "season_ranges = [\n",
    "    ('2023-12-01', '2024-01-29', 'Winter'),   # block_id=0\n",
    "    ('2024-03-15', '2024-05-13', 'Spring'),    # block_id=1\n",
    "    ('2023-07-01', '2023-08-29', 'Summer'),    # block_id=2\n",
    "    ('2023-09-15', '2023-11-13', 'Fall'),      # block_id=3\n",
    "]\n",
    "\n",
    "blocks = []\n",
    "for i, (start, end, name) in enumerate(season_ranges):\n",
    "    mask = (df_merged['time'] >= start) & (df_merged['time'] <= end)\n",
    "    block = df_merged[mask].copy()\n",
    "    block['block_id'] = i                      # 계절 블록 식별자\n",
    "    blocks.append(block)\n",
    "    print(f\"[{name}] {start} ~ {end}: {len(block):,} rows\")\n",
    "\n",
    "df_merged = pd.concat(blocks, ignore_index=True)\n",
    "print(f\"\\nTotal: {len(df_merged):,} rows (4-season block sampling, 60일 × 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ayg4l3jf0te",
   "metadata": {},
   "source": [
    "## 3. Sliding Windows 생성\n",
    "- X: (n_samples, input_time, 10) — value, temperature, rainfall, humidity + 6 cyclical features\n",
    "- y: (n_samples, output_time) — value만 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9md5y74ue97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Settings ===\n",
      "Input: 72 steps (72min = 1.2h), 10 features\n",
      "Output: 15 steps (15min) × 4 rolling = 60min\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# [설정] Sliding Window 입출력 정의 & Feature 컬럼 지정\n",
    "# ============================================================\n",
    "# feature_cols (10개): 모델 입력 X의 컬럼 순서\n",
    "#   - 물리량 4개: value, temperature, rainfall, humidity\n",
    "#   - 시간 인코딩 6개: time_sin/cos, dow_sin/cos, season_sin/cos\n",
    "# scale_cols (4개): MinMaxScaler 적용 대상 (sin/cos는 이미 [0,1])\n",
    "# target_col: 예측 대상 = value (유량)\n",
    "# input_time=72: 과거 72분(1.2시간) 입력\n",
    "# output_time=15: 미래 15분 예측 (× 4 rolling = 1시간)\n",
    "# ============================================================\n",
    "\n",
    "feature_cols = ['value', 'temperature', 'rainfall', 'humidity',\n",
    "                'time_sin', 'time_cos', 'dow_sin', 'dow_cos',\n",
    "                'season_sin', 'season_cos']\n",
    "scale_cols = ['value', 'temperature', 'rainfall', 'humidity']\n",
    "target_col = 'value'\n",
    "input_time = 72    # 72 steps × 1min = 72min\n",
    "output_time = 15   # 15 steps × 1min = 15min (× 4 rolling = 1h)\n",
    "\n",
    "print(f\"=== Settings ===\")\n",
    "print(f\"Input: {input_time} steps ({input_time}min = {input_time/60:.1f}h), {len(feature_cols)} features\")\n",
    "print(f\"Output: {output_time} steps ({output_time}min) × 4 rolling = {output_time*4}min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9g03ebo0hw",
   "metadata": {},
   "source": [
    "## 4. Train / Val / Test Split (0.7 / 0.15 / 0.15)\n",
    "- Split 경계에 gap 적용 → sliding window 겹침(data leak) 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cboyx24gome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0: 37,534 samples (train=26,273 / val=5,630 / test=5,457) gap=87\n",
      "Block 1: 47,518 samples (train=33,262 / val=7,127 / test=6,955) gap=87\n",
      "Block 2: 64,694 samples (train=45,285 / val=9,704 / test=9,531) gap=87\n",
      "Block 3: 56,652 samples (train=39,656 / val=8,497 / test=8,325) gap=87\n",
      "\n",
      "=== Sliding Window + Split (Seasonal Blocks × Segments, gap=87) ===\n",
      "★ Segment-aware: 533개 세그먼트 사용, 1487개 스킵 (< 87 steps)\n",
      "X shape: (72, 10), y shape: (15,)\n",
      "Train: 144,476 | Val: 30,958 | Test: 30,268 | Total: 205,702\n",
      "Memory: X_train 416.1 MB, y_train 8.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Sliding window 생성 + Train/Val/Test split (block × segment 독립)\n",
    "# ★ 수정: segment_id 경계를 넘는 window 생성 방지\n",
    "# split 경계에 gap 적용 → sliding window 겹침(data leak) 방지\n",
    "\n",
    "train_ratio, val_ratio = 0.7, 0.15\n",
    "split_gap = input_time + output_time  # 87 steps gap (data leak 방지)\n",
    "min_segment_len = input_time + output_time  # 세그먼트 최소 길이\n",
    "\n",
    "X_train_list, y_train_list = [], []\n",
    "X_val_list, y_val_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "test_times_list = []\n",
    "\n",
    "skipped_segments = 0\n",
    "used_segments = 0\n",
    "\n",
    "for block_id in sorted(df_merged['block_id'].unique()):\n",
    "    block = df_merged[df_merged['block_id'] == block_id]\n",
    "    \n",
    "    # ★ block 내 segment별로 sliding window 생성\n",
    "    seg_X, seg_y, seg_times = [], [], []\n",
    "    \n",
    "    for seg_id in sorted(block['segment_id'].unique()):\n",
    "        segment = block[block['segment_id'] == seg_id].reset_index(drop=True)\n",
    "        \n",
    "        if len(segment) < min_segment_len:\n",
    "            skipped_segments += 1\n",
    "            continue  # 너무 짧은 세그먼트 스킵\n",
    "        \n",
    "        used_segments += 1\n",
    "        seg_features = segment[feature_cols].values.astype(np.float32)\n",
    "        seg_target = segment[target_col].values.astype(np.float32)\n",
    "        seg_time = segment['time'].values\n",
    "        \n",
    "        n_samples = len(segment) - input_time - output_time + 1\n",
    "        for i in range(n_samples):\n",
    "            seg_X.append(seg_features[i : i + input_time])\n",
    "            seg_y.append(seg_target[i + input_time : i + input_time + output_time])\n",
    "            seg_times.append(seg_time[i + input_time])\n",
    "    \n",
    "    if len(seg_X) == 0:\n",
    "        print(f\"Block {block_id}: 유효 윈도우 없음 (스킵)\")\n",
    "        continue\n",
    "    \n",
    "    X_block = np.array(seg_X)\n",
    "    y_block = np.array(seg_y)\n",
    "    times_block = np.array(seg_times)\n",
    "    n_samples = len(X_block)\n",
    "\n",
    "    # gap을 두어 Train/Val/Test 간 window 겹침 방지\n",
    "    t_end = int(n_samples * train_ratio)\n",
    "    v_start = t_end + split_gap\n",
    "    v_end = v_start + int(n_samples * val_ratio)\n",
    "    test_start = v_end + split_gap\n",
    "\n",
    "    X_train_list.append(X_block[:t_end])\n",
    "    y_train_list.append(y_block[:t_end])\n",
    "    X_val_list.append(X_block[v_start:v_end])\n",
    "    y_val_list.append(y_block[v_start:v_end])\n",
    "    X_test_list.append(X_block[test_start:])\n",
    "    y_test_list.append(y_block[test_start:])\n",
    "\n",
    "    for i in range(test_start, n_samples):\n",
    "        test_times_list.append(times_block[i])\n",
    "\n",
    "    n_train = t_end\n",
    "    n_val = v_end - v_start\n",
    "    n_test = n_samples - test_start\n",
    "    print(f\"Block {block_id}: {n_samples:,} samples \"\n",
    "          f\"(train={n_train:,} / val={n_val:,} / test={n_test:,}) \"\n",
    "          f\"gap={split_gap}\")\n",
    "\n",
    "X_train = np.concatenate(X_train_list)\n",
    "y_train = np.concatenate(y_train_list)\n",
    "X_val = np.concatenate(X_val_list)\n",
    "y_val = np.concatenate(y_val_list)\n",
    "X_test = np.concatenate(X_test_list)\n",
    "y_test = np.concatenate(y_test_list)\n",
    "test_times = np.array(test_times_list)\n",
    "\n",
    "n_total = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"\\n=== Sliding Window + Split (Seasonal Blocks × Segments, gap={split_gap}) ===\")\n",
    "print(f\"★ Segment-aware: {used_segments}개 세그먼트 사용, {skipped_segments}개 스킵 (< {min_segment_len} steps)\")\n",
    "print(f\"X shape: ({input_time}, {len(feature_cols)}), y shape: ({output_time},)\")\n",
    "print(f\"Train: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,} | Total: {n_total:,}\")\n",
    "print(f\"Memory: X_train {X_train.nbytes/1e6:.1f} MB, y_train {y_train.nbytes/1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7qtwynq7uec",
   "metadata": {},
   "source": [
    "## 5. 정규화 (Train 기준)\n",
    "- X: feature별 개별 MinMaxScaler (value, temperature, rainfall, humidity)\n",
    "- y: value scaler로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de9h20c77vf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 기준 Scaler 범위 (scale_cols만):\n",
      "         value: [0.00, 310.26]\n",
      "   temperature: [-11.70, 37.70]\n",
      "      rainfall: [0.00, 54.00]\n",
      "      humidity: [4.80, 99.90]\n",
      "\n",
      "sin/cos features (6개): 정규화 없이 원본 유지 [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# [정규화] Train 기준 MinMaxScaler (0~1 스케일링)\n",
    "# ============================================================\n",
    "# - Train 데이터로만 scaler fit → Val/Test에 동일 scaler 적용 (data leak 방지)\n",
    "# - scale_cols(4개)만 정규화: value, temperature, rainfall, humidity\n",
    "# - sin/cos features(6개)는 이미 [0,1] 범위이므로 정규화 불필요\n",
    "# - normalize_y / denormalize_y: 예측값 ↔ 원본 스케일 변환 함수\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "n_features = len(feature_cols)\n",
    "\n",
    "# --- Train 데이터 기준으로 scaler fit ---\n",
    "scalers = {}\n",
    "for col in scale_cols:\n",
    "    i = feature_cols.index(col)          # feature_cols 내 인덱스\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train[:, :, i].reshape(-1, 1))  # Train의 해당 feature 전체로 fit\n",
    "    scalers[col] = scaler\n",
    "\n",
    "# --- X 정규화 함수 (scale_cols만, sin/cos 제외) ---\n",
    "def normalize_X(arr):\n",
    "    \"\"\"X 배열의 scale_cols만 [0,1]로 정규화한다.\"\"\"\n",
    "    arr = arr.copy()\n",
    "    for col in scale_cols:\n",
    "        i = feature_cols.index(col)\n",
    "        s = scalers[col]\n",
    "        d_min, d_max = np.float32(s.data_min_[0]), np.float32(s.data_max_[0])\n",
    "        arr[:, :, i] = (arr[:, :, i] - d_min) / (d_max - d_min)\n",
    "    return arr\n",
    "\n",
    "X_train_scaled = normalize_X(X_train)\n",
    "X_val_scaled = normalize_X(X_val)\n",
    "X_test_scaled = normalize_X(X_test)\n",
    "\n",
    "# --- y 정규화/역정규화 함수 (value scaler 사용) ---\n",
    "val_min = np.float32(scalers['value'].data_min_[0])\n",
    "val_max = np.float32(scalers['value'].data_max_[0])\n",
    "\n",
    "def normalize_y(arr):\n",
    "    \"\"\"y값을 [0,1]로 정규화한다.\"\"\"\n",
    "    return (arr - val_min) / (val_max - val_min)\n",
    "\n",
    "def denormalize_y(arr):\n",
    "    \"\"\"정규화된 y값을 원본 스케일로 복원한다.\"\"\"\n",
    "    return arr * (val_max - val_min) + val_min\n",
    "\n",
    "y_train_scaled = normalize_y(y_train)\n",
    "y_val_scaled = normalize_y(y_val)\n",
    "y_test_scaled = normalize_y(y_test)\n",
    "\n",
    "print(f\"Train 기준 Scaler 범위 (scale_cols만):\")\n",
    "for col in scale_cols:\n",
    "    s = scalers[col]\n",
    "    print(f\"  {col:>12s}: [{s.data_min_[0]:.2f}, {s.data_max_[0]:.2f}]\")\n",
    "print(f\"\\nsin/cos features ({n_features - len(scale_cols)}개): 정규화 없이 원본 유지 [0, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4164181",
   "metadata": {},
   "source": [
    "- Tensor 변환 & DataLoader (pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc1f89f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Train: 564 batches | Val: 61 | Test: 60\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# [Tensor 변환 & DataLoader] NumPy → PyTorch Tensor → DataLoader\n",
    "# ============================================================\n",
    "# - FloatTensor: float32 형태로 GPU 연산 최적화\n",
    "# - pin_memory=True (CUDA): CPU→GPU 전송 속도 향상 (페이지 잠금 메모리)\n",
    "# - Train: shuffle=True, drop_last=True (마지막 불완전 배치 제거)\n",
    "# - Val/Test: shuffle=False (순서 유지), 큰 batch_size(512)로 빠른 평가\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# --- NumPy → PyTorch Tensor ---\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train_scaled)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val_scaled)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test_scaled)\n",
    "\n",
    "# --- TensorDataset 생성 ---\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# --- DataLoader 생성 ---\n",
    "batch_size = 256\n",
    "use_pin = (device.type == 'cuda')  # GPU 사용 시 pin_memory 활성화\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          drop_last=True, pin_memory=use_pin, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False,\n",
    "                        pin_memory=use_pin, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False,\n",
    "                         pin_memory=use_pin, num_workers=0)\n",
    "\n",
    "print(f\"Train: {len(train_loader)} batches | Val: {len(val_loader)} | Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18826bfa",
   "metadata": {},
   "source": [
    "## 6. LSTMSeq2SeqAttnModel 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e42ea150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# [모델 정의] LSTMSeq2SeqAttnModel — Encoder-Decoder + Attention\n",
    "# ============================================================\n",
    "# 구조:\n",
    "#   Encoder: 2-layer LSTM → 입력 시퀀스(72 steps)를 hidden state로 압축\n",
    "#   Attention: Bahdanau(Additive) 방식\n",
    "#     - Wh(decoder hidden) + Ws(encoder outputs) → tanh → V → softmax\n",
    "#     - encoder의 72개 time step 중 중요한 부분에 가중치 집중\n",
    "#   Decoder: LSTMCell × 15 steps (autoregressive)\n",
    "#     - 각 step마다: attention context + step embedding → LSTMCell → 예측\n",
    "#     - step_embedding: 디코더가 \"현재 몇 번째 step인지\" 인식 (위치 정보)\n",
    "#   출력 헤드: LayerNorm → Dropout → Linear(1)\n",
    "#\n",
    "# 파라미터:\n",
    "#   input_size=10 (features), hidden_size=128, num_layers=2\n",
    "#   output_size=15 (예측 steps), embed_dim=16, dropout=0.2\n",
    "# ============================================================\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMSeq2SeqAttnModel(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden_size=128, num_layers=2,\n",
    "                 output_size=15, embed_dim=16, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # --- Encoder: 2-layer Bidirectional-free LSTM ---\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_size, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0  # layer간 dropout\n",
    "        )\n",
    "\n",
    "        # --- Step Embedding: 디코더 step 위치 정보 (0~14 → 16dim 벡터) ---\n",
    "        self.step_embedding = nn.Embedding(output_size, embed_dim)\n",
    "\n",
    "        # --- Bahdanau Attention ---\n",
    "        self.attn_Wh = nn.Linear(hidden_size, hidden_size, bias=False)  # decoder query 변환\n",
    "        self.attn_Ws = nn.Linear(hidden_size, hidden_size, bias=False)  # encoder key 변환\n",
    "        self.attn_V = nn.Linear(hidden_size, 1, bias=False)             # energy → scalar\n",
    "\n",
    "        # --- Decoder: LSTMCell (step별 순차 실행) ---\n",
    "        self.decoder = nn.LSTMCell(\n",
    "            input_size=hidden_size + embed_dim,  # context(128) + step_emb(16)\n",
    "            hidden_size=hidden_size\n",
    "        )\n",
    "\n",
    "        # --- 출력 헤드: LayerNorm → Dropout → FC ---\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, 1)  # hidden → 유량 1값\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # --- Encoder ---\n",
    "        enc_outputs, (h_n, c_n) = self.encoder(x)      # enc_outputs: (B, 72, 128)\n",
    "        enc_keys_projected = self.attn_Ws(enc_outputs)  # key 사전 계산 (효율화)\n",
    "\n",
    "        # Decoder 초기 상태 = Encoder 마지막 layer의 hidden/cell\n",
    "        h_dec = h_n[-1]  # (B, 128)\n",
    "        c_dec = c_n[-1]  # (B, 128)\n",
    "\n",
    "        predictions = []\n",
    "        self.attn_weights_all = []\n",
    "        step_ids = torch.arange(self.output_size, device=x.device)\n",
    "        step_embs = self.step_embedding(step_ids)  # (15, 16)\n",
    "\n",
    "        # --- Decoder: 15 steps 순차 예측 ---\n",
    "        for t in range(self.output_size):\n",
    "            # Attention: query(decoder) × key(encoder) → weight → context\n",
    "            query = self.attn_Wh(h_dec).unsqueeze(1)             # (B, 1, 128)\n",
    "            energy = torch.tanh(query + enc_keys_projected)       # (B, 72, 128)\n",
    "            score = self.attn_V(energy).squeeze(-1)               # (B, 72)\n",
    "            attn_weights = torch.softmax(score, dim=1)            # (B, 72) 정규화\n",
    "            context = torch.bmm(attn_weights.unsqueeze(1), enc_outputs).squeeze(1)  # (B, 128)\n",
    "            self.attn_weights_all.append(attn_weights.detach())\n",
    "\n",
    "            # Decoder input = attention context + step embedding\n",
    "            step_emb = step_embs[t].unsqueeze(0).expand(batch_size, -1)  # (B, 16)\n",
    "            dec_input = torch.cat([context, step_emb], dim=1)            # (B, 144)\n",
    "            h_dec, c_dec = self.decoder(dec_input, (h_dec, c_dec))\n",
    "\n",
    "            # 출력: LayerNorm → Dropout → FC\n",
    "            pred_t = self.fc_out(self.layer_norm(h_dec))   # (B, 1)\n",
    "            predictions.append(pred_t)\n",
    "\n",
    "        predictions = torch.cat(predictions, dim=1)                       # (B, 15)\n",
    "        self.attn_weights_all = torch.stack(self.attn_weights_all, dim=1) # (B, 15, 72)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1b3f5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 377,585\n"
     ]
    }
   ],
   "source": [
    "# [확인] 모델 인스턴스 생성 → 총 파라미터 수 확인 후 삭제\n",
    "model_name = \"Ablation_B\"\n",
    "model_check = LSTMSeq2SeqAttnModel(\n",
    "    input_size=n_features, hidden_size=128, num_layers=2,\n",
    "    output_size=output_time, embed_dim=16, dropout=0.2,\n",
    ")\n",
    "total_p = sum(p.numel() for p in model_check.parameters())\n",
    "print(f\"Parameters: {total_p:,}\")\n",
    "del model_check  # 메모리 해제 (학습 시 새로 생성)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f9b45",
   "metadata": {},
   "source": [
    "## 7. Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88fd80c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# [EarlyStopping] 과적합 방지를 위한 조기 종료 클래스\n",
    "# ============================================================\n",
    "# - patience 횟수 동안 val_loss가 min_delta 이상 개선되지 않으면 학습 중단\n",
    "# - best_model: 가장 낮은 val_loss 시점의 모델 가중치를 저장\n",
    "# - __call__(): 매 epoch 후 호출 → 개선 시 저장, 미개선 시 카운터 증가\n",
    "# ============================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=1e-5, verbose=True):\n",
    "        self.patience = patience      # 허용할 연속 미개선 epoch 수\n",
    "        self.min_delta = min_delta    # 개선으로 인정할 최소 감소량\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0              # 연속 미개선 카운터\n",
    "        self.best_loss: float | None = None\n",
    "        self.early_stop = False       # True가 되면 학습 루프 탈출\n",
    "        self.best_model: dict[str, torch.Tensor] | None = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            # 첫 호출: 현재 loss를 best로 설정\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            # 개선됨: best 갱신, 카운터 리셋\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "            \n",
    "        else:\n",
    "            # 미개선: 카운터 증가, patience 초과 시 early_stop 플래그 설정\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"현재 모델 가중치를 best_model에 복사 (deepcopy).\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f}). Saving model...')\n",
    "        self.best_model = model.state_dict().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc1216",
   "metadata": {},
   "source": [
    "## 8. 학습 (Multi-Run, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f987a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# [학습 설정] 하이퍼파라미터 & Step-Weighted MSE Loss\n",
    "# ============================================================\n",
    "# - num_epochs=100: 최대 epoch (EarlyStopping으로 조기 종료)\n",
    "# - learning_rate=0.001: Adam 초기 학습률\n",
    "# - es_patience=10: EarlyStopping patience\n",
    "# - step_weights: step 1→1.0, step 15→2.0 (선형 증가)\n",
    "#   먼 미래일수록 예측이 어려우므로 가중치를 높여 학습 강조\n",
    "# - weighted_mse_loss: step별 가중 MSE → 단일 scalar loss\n",
    "# ============================================================\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "es_patience = 10\n",
    "\n",
    "# Step-weighted: 가까운 step(1.0) → 먼 step(2.0) 선형 증가 가중치\n",
    "step_weights = torch.linspace(1.0, 2.0, output_time).to(device)\n",
    "\n",
    "def weighted_mse_loss(pred, target):\n",
    "    \"\"\"Step별 가중치를 곱한 MSE Loss.\"\"\"\n",
    "    return (step_weights * (pred - target) ** 2).mean()\n",
    "\n",
    "criterion = weighted_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59245504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Run 1/3 (seed=42)\n",
      "============================================================\n",
      "  Epoch [ 10/100] Train: 0.001550 Val: 0.001298 LR: 0.001000\n",
      "  Epoch [ 20/100] Train: 0.001288 Val: 0.001120 LR: 0.000500\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# [Multi-Run 학습] N_RUNS(3)회 반복 학습 + Test 평가\n",
    "# ============================================================\n",
    "# 각 Run에서:\n",
    "#   1) set_seed()로 재현성 보장\n",
    "#   2) 모델/옵티마이저/스케줄러/EarlyStopping 초기화\n",
    "#   3) 학습 루프: Train → Val → LR schedule → EarlyStopping 체크\n",
    "#   4) Best 모델 복원 후 Test 추론\n",
    "#   5) 원본 스케일로 역정규화 → MAPE/sMAPE/RMSE/MAE/R²/Bias 산출\n",
    "#   6) Step별 MAPE, 계절별 MAPE, Macro MAPE 기록\n",
    "#\n",
    "# 학습 구성:\n",
    "#   - Optimizer: Adam (lr=0.001, weight_decay=1e-5)\n",
    "#   - LR Scheduler: ReduceLROnPlateau (factor=0.5, patience=3)\n",
    "#   - Gradient Clipping: max_norm=1.0\n",
    "#   - EarlyStopping: patience=10\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def calc_mape(y_true, y_pred):\n",
    "    \"\"\"MAPE(%) — y_true=0인 지점 제외.\"\"\"\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def calc_smape(y_true, y_pred):\n",
    "    \"\"\"sMAPE(%) — 대칭 MAPE, 분모=(|실측|+|예측|)/2.\"\"\"\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    mask = denom > 0\n",
    "    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / denom[mask]) * 100\n",
    "\n",
    "all_run_results = []   # 각 run의 평가 결과 dict\n",
    "all_run_models = []    # 각 run의 best model state_dict\n",
    "all_run_losses = []    # 각 run의 train/val loss 이력\n",
    "\n",
    "for run_idx, seed in enumerate(SEEDS[:N_RUNS]):\n",
    "    set_seed(seed)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Run {run_idx+1}/{N_RUNS} (seed={seed})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # --- 모델 & 옵티마이저 & 스케줄러 초기화 ---\n",
    "    model = LSTMSeq2SeqAttnModel(\n",
    "        input_size=n_features, hidden_size=128, num_layers=2,\n",
    "        output_size=output_time, embed_dim=16, dropout=0.2,\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3  # val_loss 3 epoch 미개선 → lr ×0.5\n",
    "    )\n",
    "    early_stopping = EarlyStopping(patience=es_patience, verbose=False)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # --- 학습 루프 ---\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train phase\n",
    "        model.train()\n",
    "        train_loss_epoch = 0.0\n",
    "        for batch_X, targets in train_loader:\n",
    "            batch_X, targets = batch_X.to(device), targets.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # gradient clipping\n",
    "            optimizer.step()\n",
    "            train_loss_epoch += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss_epoch = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, targets in val_loader:\n",
    "                batch_X, targets = batch_X.to(device), targets.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss_epoch += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss_epoch / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # LR Scheduler & 로깅\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'  Epoch [{epoch+1:3d}/{num_epochs}] '\n",
    "                  f'Train: {avg_train_loss:.6f} '\n",
    "                  f'Val: {avg_val_loss:.6f} '\n",
    "                  f'LR: {current_lr:.6f}')\n",
    "        \n",
    "        # EarlyStopping 체크\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"  Early Stopping at Epoch {epoch+1} \"\n",
    "                  f\"(Best Val Loss: {early_stopping.best_loss:.6f})\")\n",
    "            break\n",
    "    \n",
    "    # --- Best 모델 복원 ---\n",
    "    if early_stopping.best_model is not None:\n",
    "        model.load_state_dict(early_stopping.best_model)\n",
    "    \n",
    "    all_run_models.append(copy.deepcopy(model.state_dict()))\n",
    "    all_run_losses.append({'train': train_losses, 'val': val_losses})\n",
    "    \n",
    "    # --- Test 추론 & 역정규화 ---\n",
    "    model.eval()\n",
    "    test_preds_list, test_actuals_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            test_preds_list.append(outputs.cpu().numpy())\n",
    "            test_actuals_list.append(batch_y.numpy())\n",
    "    \n",
    "    run_preds = denormalize_y(np.vstack(test_preds_list))     # 원본 스케일 복원\n",
    "    run_actuals = denormalize_y(np.vstack(test_actuals_list))\n",
    "    \n",
    "    # --- 전체 지표 산출 ---\n",
    "    run_result = {\n",
    "        'seed': seed,\n",
    "        'epoch': len(train_losses),\n",
    "        'mape': calc_mape(run_actuals.flatten(), run_preds.flatten()),\n",
    "        'smape': calc_smape(run_actuals.flatten(), run_preds.flatten()),\n",
    "        'rmse': np.sqrt(mean_squared_error(run_actuals, run_preds)),\n",
    "        'mae': mean_absolute_error(run_actuals, run_preds),\n",
    "        'r2': r2_score(run_actuals.flatten(), run_preds.flatten()),\n",
    "        'bias': float(np.mean(run_actuals - run_preds)),  # 양수=과소예측\n",
    "        'predictions': run_preds,\n",
    "        'actuals': run_actuals,\n",
    "    }\n",
    "    \n",
    "    # --- Step별 MAPE (1~15분 각각) ---\n",
    "    step_mapes = []\n",
    "    for s in range(output_time):\n",
    "        step_mapes.append(calc_mape(run_actuals[:, s], run_preds[:, s]))\n",
    "    run_result['step_mapes'] = step_mapes\n",
    "    \n",
    "    # --- 계절별 MAPE (block 단위) ---\n",
    "    season_mapes = []\n",
    "    offset = 0\n",
    "    for size in [len(arr) for arr in X_test_list]:\n",
    "        block_a = run_actuals[offset:offset+size]\n",
    "        block_p = run_preds[offset:offset+size]\n",
    "        season_mapes.append(calc_mape(block_a.flatten(), block_p.flatten()))\n",
    "        offset += size\n",
    "    run_result['season_mapes'] = season_mapes\n",
    "    run_result['macro_mape'] = np.mean(season_mapes)  # 계절 균등 평균\n",
    "    \n",
    "    all_run_results.append(run_result)\n",
    "    \n",
    "    print(f\"  → MAPE: {run_result['mape']:.2f}% | SMAPE: {run_result['smape']:.2f}% \"\n",
    "          f\"| R²: {run_result['r2']:.4f} | Bias: {run_result['bias']:+.2f} \"\n",
    "          f\"| Epoch: {run_result['epoch']}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All runs completed\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b8ac13",
   "metadata": {},
   "source": [
    "## 9. 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd845c",
   "metadata": {},
   "source": [
    "### Step별 정확도 (특정 시점 조회)\n",
    "- `step_accuracy(step)`: 특정 예측 시점(1~15분)의 정확도 지표 반환\n",
    "- 정확도 = `100 - MAPE(%)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6pu9i7j7lxn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# [평가] Best Run 선택 & Step별 정확도 함수\n",
    "# ============================================================\n",
    "# - MAPE가 가장 낮은 run을 best로 선택\n",
    "# - step_accuracy(step): 특정 예측 시점(1~15분)의 정확도 지표 계산\n",
    "#   정확도 = 100 - MAPE(%)\n",
    "# - 전체 Step 요약 테이블 출력\n",
    "# ============================================================\n",
    "\n",
    "best_idx = int(np.argmin([r['mape'] for r in all_run_results]))\n",
    "best_run = all_run_results[best_idx]\n",
    "preds = best_run['predictions']   # (n_test, 15) 역정규화된 예측값\n",
    "actuals = best_run['actuals']     # (n_test, 15) 역정규화된 실측값\n",
    "\n",
    "print(f\"Best Run: seed={best_run['seed']} (Run {best_idx+1})\")\n",
    "print(f\"Test samples: {len(preds):,}\\n\")\n",
    "\n",
    "def step_accuracy(step):\n",
    "    \"\"\"\n",
    "    특정 예측 시점(step)의 정확도 지표를 계산한다.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    step : int\n",
    "        조회할 예측 시점 (1~15). step=1 → +1분, step=15 → +15분\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict  {'step', 'time', 'accuracy', 'mape', 'smape', 'rmse', 'mae', 'r2'}\n",
    "        - accuracy = 100 - MAPE (%)\n",
    "    \"\"\"\n",
    "    assert 1 <= step <= actuals.shape[1], f\"step 범위: 1~{actuals.shape[1]}\"\n",
    "    a = actuals[:, step - 1]\n",
    "    p = preds[:, step - 1]\n",
    "\n",
    "    mape  = calc_mape(a, p)\n",
    "    smape = calc_smape(a, p)\n",
    "    rmse  = np.sqrt(mean_squared_error(a, p))\n",
    "    mae   = mean_absolute_error(a, p)\n",
    "    r2    = r2_score(a, p)\n",
    "\n",
    "    return {\n",
    "        'step': step,\n",
    "        'time': f'+{step}min',\n",
    "        'accuracy': 100 - mape,\n",
    "        'mape': mape,\n",
    "        'smape': smape,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "    }\n",
    "\n",
    "# --- 전체 Step 요약 테이블 ---\n",
    "rows = [step_accuracy(s) for s in range(1, output_time + 1)]\n",
    "df_steps = pd.DataFrame(rows).set_index('step')\n",
    "\n",
    "print(\"=\" * 72)\n",
    "print(f\"  Step별 정확도  (Best Run, seed={best_run['seed']})\")\n",
    "print(\"=\" * 72)\n",
    "print(df_steps.to_string(float_format='%.2f'))\n",
    "print(f\"\\n전체 평균 정확도: {df_steps['accuracy'].mean():.2f}%\")\n",
    "print(f\"전체 MAPE:       {best_run['mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dms08j8ttb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# [특정 시점 조회] query_step 변경으로 원하는 시점의 정확도 확인\n",
    "# ============================================================\n",
    "# 사용법: query_step = 원하는 값(1~15)으로 변경 후 셀 실행\n",
    "# 예) query_step = 1  → +1분 시점\n",
    "#     query_step = 5  → +5분 시점\n",
    "#     query_step = 15 → +15분 시점\n",
    "# ============================================================\n",
    "\n",
    "query_step = 15\n",
    "\n",
    "result = step_accuracy(query_step)\n",
    "\n",
    "print(f\"━━━ +{query_step}min 시점 예측 정확도 ━━━\")\n",
    "print(f\"  정확도 (100-MAPE) : {result['accuracy']:.2f}%\")\n",
    "print(f\"  MAPE              : {result['mape']:.2f}%\")\n",
    "print(f\"  sMAPE             : {result['smape']:.2f}%\")\n",
    "print(f\"  RMSE              : {result['rmse']:.2f}\")\n",
    "print(f\"  MAE               : {result['mae']:.2f}\")\n",
    "print(f\"  R²                : {result['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biww7a6r1oc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# [모델 저장] Best Run의 모델 가중치 + 메타데이터를 .pth로 저장\n",
    "# ============================================================\n",
    "# 저장 항목:\n",
    "#   - model_state_dict: 학습된 모델 가중치\n",
    "#   - scalers: 정규화 범위 (추론 시 동일 스케일링 적용 필요)\n",
    "#   - feature_cols / scale_cols: 입력 컬럼 구성\n",
    "#   - input_time / output_time: 입출력 시퀀스 길이\n",
    "#   - train/val losses, best seed/epoch: 학습 이력\n",
    "# ============================================================\n",
    "\n",
    "model_path = BASE_DIR / \"models\" / \"ablation_b_dropout_removed.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'model_name': model_name,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'scalers': {col: {'min': scalers[col].data_min_[0], 'max': scalers[col].data_max_[0]} for col in scale_cols},\n",
    "    'feature_cols': feature_cols,\n",
    "    'scale_cols': scale_cols,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'test_loss': best_run['mape'],\n",
    "    'best_epoch': len(train_losses),\n",
    "    'best_seed': best_run['seed'],\n",
    "    'input_time': input_time,\n",
    "    'output_time': output_time,\n",
    "}, model_path)\n",
    "\n",
    "print(f\"모델 저장 완료: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "win_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
