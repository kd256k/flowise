{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8efcd086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8386f86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.10.0+cu126\n",
      "CUDA 사용 가능: True\n",
      "CUDA: 12.6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec282b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 2.4.2\n",
      "pandas: 3.0.0\n",
      "scikit-learn: 1.8.0\n",
      "torch: 2.10.0+cu126\n"
     ]
    }
   ],
   "source": [
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"torch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974614a0",
   "metadata": {},
   "source": [
    "1. 데이터 불러오기\n",
    "2. 데이터 전처리\n",
    "3. 데이터 분할\n",
    "4. 데이터 정규화\n",
    "---\n",
    "5. 학습/검증/테스트 데이터(텐서로 변경해야 함)\n",
    "6. 모델 생성\n",
    "7. 학습\n",
    "8. 예측\n",
    "9. 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1463ed9d",
   "metadata": {},
   "source": [
    "0. 데이터 크기 설정\n",
    "1. 데이터 불러오기\n",
    "2. 전처리 (IQR, Savgol)\n",
    "3. Sliding Windows 생성\n",
    "4. Train/Val/Test Split \n",
    "5. scaler.fit(Train만)  # ← 핵심!\n",
    "6. Train: scaler.transform(Train)\n",
    "7. Val: scaler.transform(Val)\n",
    "8. Test: scaler.transform(Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62bac7e",
   "metadata": {},
   "source": [
    "0. 데이터 크기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c823f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 설정 ===\n",
      "입력: 144분 (0.1일)\n",
      "출력: 10분 (0.0일)\n",
      "최소 필요: 154분 (0.1일)\n"
     ]
    }
   ],
   "source": [
    "input_time = 144\n",
    "output_time = 10\n",
    "min_required = input_time + output_time\n",
    "\n",
    "print(f\"=== 설정 ===\")\n",
    "print(f\"입력: {input_time}분 ({input_time/60/24:.1f}일)\")\n",
    "print(f\"출력: {output_time}분 ({output_time/60/24:.1f}일)\")\n",
    "print(f\"최소 필요: {min_required}분 ({min_required/60/24:.1f}일)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b373ce",
   "metadata": {},
   "source": [
    "1. 데이터 불러오기\n",
    "- J배수지 \"csv.10 \" 사용\n",
    "- load 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5bb2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9867401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/user/MainProject/project')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "567eeca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = BASE_DIR / \"data\" / \"rawdata\" / \"reservoir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    csv_file/\"10.csv\", \n",
    "    header=None, \n",
    "    usecols=[1, 2], \n",
    "    names=['time', 'value']\n",
    ").sort_values('time').reset_index(drop=True)\n",
    "\n",
    "# 시간 변환 (잘못된 형식은 NaT로 처리)\n",
    "df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "\n",
    "# 잘못된 시간 데이터 제거\n",
    "before_clean = len(df)\n",
    "df = df.dropna(subset=['time'])\n",
    "if before_clean > len(df):\n",
    "    print(f\"⚠️  잘못된 시간 형식 제거: {before_clean - len(df)}개\")\n",
    "\n",
    "print(f\"\\n=== 데이터 확인 ====\")\n",
    "print(f\"원본 데이터: {len(df)}개\")\n",
    "print(f\"기간: {df['time'].min()} ~ {df['time'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e6b7a",
   "metadata": {},
   "source": [
    "2. 데이터 전처리\n",
    "- IQR\n",
    "- 결측치 보간 (interpolate)\n",
    "- Savgol filter 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc5b76c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제거 내역:\n",
      "  IQR: 21개\n",
      "  음수: 0개\n",
      "  급변동 (>91.06): 944개\n",
      "  총 제거: 962개 (중복 제외)\n",
      "\n",
      "처리 후 범위: [0.00, 326.91]\n",
      "음수 값: 0개\n",
      "1분 변동량 최대: 105.08\n",
      "IQR 후: 943434개\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_filtered = df.copy()\n",
    "\n",
    "# 1. IQR 이상치 제거\n",
    "Q1 = df_filtered[\"value\"].quantile(0.25)\n",
    "Q3 = df_filtered[\"value\"].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "upper = Q3 + 1.5 * IQR\n",
    "lower = Q1 - 1.5 * IQR\n",
    "iqr_mask = (df_filtered['value'] < lower) | (df_filtered['value'] > upper)\n",
    "\n",
    "# 2. 음수 값 (센서 오류)\n",
    "negative_mask = df_filtered['value'] < 0\n",
    "\n",
    "# 급격한 변동 (상위 0.1%)\n",
    "diff = df_filtered['value'].diff().abs()\n",
    "spike_threshold = diff.quantile(0.999)\n",
    "spike_mask = diff > spike_threshold\n",
    "\n",
    "# 한 번에 제거\n",
    "total_mask = iqr_mask | negative_mask | spike_mask\n",
    "df_filtered.loc[total_mask, 'value'] = np.nan\n",
    "\n",
    "print(f\"제거 내역:\")\n",
    "print(f\"  IQR: {iqr_mask.sum()}개\")\n",
    "print(f\"  음수: {negative_mask.sum()}개\")\n",
    "print(f\"  급변동 (>{spike_threshold:.2f}): {spike_mask.sum()}개\")\n",
    "print(f\"  총 제거: {total_mask.sum()}개 (중복 제외)\")\n",
    "\n",
    "# 보간 1회만 수행\n",
    "df_filtered['value'] = df_filtered['value'].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "# 보간으로 생성 된 음수 방지\n",
    "df_filtered['value'] = df_filtered['value'].clip(lower=0)\n",
    "\n",
    "print(f\"\\n처리 후 범위: [{df_filtered['value'].min():.2f}, {df_filtered['value'].max():.2f}]\")\n",
    "print(f\"음수 값: {(df_filtered['value'] < 0).sum()}개\")\n",
    "print(f\"1분 변동량 최대: {df_filtered['value'].diff().abs().max():.2f}\")\n",
    "print(f\"IQR 후: {len(df_filtered)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f4f13dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>943434</td>\n",
       "      <td>943434.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2023-11-24 20:51:28.765622</td>\n",
       "      <td>151.434546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2023-01-01 00:01:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2023-06-13 22:18:15</td>\n",
       "      <td>106.851175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2023-11-24 20:19:30</td>\n",
       "      <td>138.765500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2024-05-06 19:36:45</td>\n",
       "      <td>194.866350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-17 17:19:00</td>\n",
       "      <td>326.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>59.043205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             time          value\n",
       "count                      943434  943434.000000\n",
       "mean   2023-11-24 20:51:28.765622     151.434546\n",
       "min           2023-01-01 00:01:00       0.000000\n",
       "25%           2023-06-13 22:18:15     106.851175\n",
       "50%           2023-11-24 20:19:30     138.765500\n",
       "75%           2024-05-06 19:36:45     194.866350\n",
       "max           2024-10-17 17:19:00     326.906000\n",
       "std                           NaN      59.043205"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe003a2",
   "metadata": {},
   "source": [
    "savgol_filter 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06c46bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Savgol 적용 (window=51)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# 백업\n",
    "df_before_savgol = df_filtered.copy()\n",
    "\n",
    "# Savgol 필터\n",
    "window_length = 51  # 홀수여야 함\n",
    "polyorder = 2\n",
    "\n",
    "df_filtered['value'] = savgol_filter(\n",
    "    df_filtered['value'].clip(lower=0), \n",
    "    window_length=window_length,\n",
    "    polyorder=polyorder\n",
    ")\n",
    "\n",
    "df_filtered['value'] = df_filtered['value'].clip(lower=0)\n",
    "\n",
    "print(f\"Savgol 적용 (window={window_length})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cd4fb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>943434</td>\n",
       "      <td>943434.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2023-11-24 20:51:28.765622</td>\n",
       "      <td>151.434605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2023-01-01 00:01:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2023-06-13 22:18:15</td>\n",
       "      <td>108.786774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2023-11-24 20:19:30</td>\n",
       "      <td>144.369784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2024-05-06 19:36:45</td>\n",
       "      <td>192.382534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-17 17:19:00</td>\n",
       "      <td>325.516638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>56.885878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             time          value\n",
       "count                      943434  943434.000000\n",
       "mean   2023-11-24 20:51:28.765622     151.434605\n",
       "min           2023-01-01 00:01:00       0.000000\n",
       "25%           2023-06-13 22:18:15     108.786774\n",
       "50%           2023-11-24 20:19:30     144.369784\n",
       "75%           2024-05-06 19:36:45     192.382534\n",
       "max           2024-10-17 17:19:00     325.516638\n",
       "std                           NaN      56.885878"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f22deb",
   "metadata": {},
   "source": [
    "[이상치 제거 확인용]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57e75670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Savgol 적용 효과\n",
      "============================================================\n",
      "Before:\n",
      "  평균 변화: 4.2279\n",
      "  최대 변화: 105.0840\n",
      "  표준편차: 9.4115\n",
      "\n",
      "After:\n",
      "  평균 변화: 1.0909\n",
      "  최대 변화: 9.9540\n",
      "  표준편차: 1.6135\n",
      "\n",
      "변동성 감소: 82.86%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# 1. 시계열 비교\n",
    "axes[0].plot(df_before_savgol['value'], \n",
    "            label='Before Savgol (IQR Only)', alpha=0.6, linewidth=1.5, color='blue')\n",
    "axes[0].plot(df_filtered['value'], \n",
    "            label='After Savgol (IQR + Savgol)', linewidth=2, color='green')\n",
    "axes[0].set_ylabel('Value', fontsize=12)\n",
    "axes[0].set_title('Before vs After Savgol Filter', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. 변화율 비교\n",
    "diff_before = np.diff(df_before_savgol['value'])\n",
    "diff_after = np.diff(df_filtered['value'])\n",
    "\n",
    "axes[1].plot(diff_before, label='Before Savgol', alpha=0.6, linewidth=1, color='blue')\n",
    "axes[1].plot(diff_after, label='After Savgol', linewidth=1.5, color='green')\n",
    "axes[1].axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1].set_xlabel('Time (Minutes)', fontsize=12)\n",
    "axes[1].set_ylabel('Change (Δ)', fontsize=12)\n",
    "axes[1].set_title('1-Minute Rate of Change', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.close()\n",
    "# ========== 통계 ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Savgol 적용 효과\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Before:\")\n",
    "print(f\"  평균 변화: {np.mean(np.abs(diff_before)):.4f}\")\n",
    "print(f\"  최대 변화: {np.max(np.abs(diff_before)):.4f}\")\n",
    "print(f\"  표준편차: {np.std(diff_before):.4f}\")\n",
    "\n",
    "print(f\"\\nAfter:\")\n",
    "print(f\"  평균 변화: {np.mean(np.abs(diff_after)):.4f}\")\n",
    "print(f\"  최대 변화: {np.max(np.abs(diff_after)):.4f}\")\n",
    "print(f\"  표준편차: {np.std(diff_after):.4f}\")\n",
    "\n",
    "reduction = (1 - np.std(diff_after) / np.std(diff_before)) * 100\n",
    "print(f\"\\n변동성 감소: {reduction:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 백업 삭제\n",
    "del df_before_savgol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7920fe4",
   "metadata": {},
   "source": [
    "통계 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 저장 전 유효성 검사\n",
    "print(f\"\\n=== 데이터 저장 ===\")\n",
    "print(f\"저장할 데이터: {len(df_filtered)}개\")\n",
    "print(f\"time 타입: {df_filtered['time'].dtype}\")\n",
    "print(f\"NaT 개수: {df_filtered['time'].isna().sum()}\")\n",
    "\n",
    "# NaT가 있다면 제거\n",
    "if df_filtered['time'].isna().sum() > 0:\n",
    "    before = len(df_filtered)\n",
    "    df_filtered = df_filtered.dropna(subset=['time'])\n",
    "    print(f\"⚠️  NaT 제거: {before - len(df_filtered)}개\")\n",
    "\n",
    "df_filtered.to_csv(\"../data/processed/flow_preprocessed.csv\", index=False)\n",
    "print(\"✅ 저장 완료: flow_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f5bd40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "데이터 전처리 통계 요약 (IQR 1회만 적용)\n",
      "============================================================\n",
      "\n",
      "원본 데이터:\n",
      "  개수: 943,434\n",
      "  범위: [0.00, 428.61]\n",
      "  평균: 151.45 ± 59.08\n",
      "\n",
      "IQR 후 (최종):\n",
      "  개수: 943,434\n",
      "  제거: 0개 (0.00%)\n",
      "  범위: [0.00, 325.52]\n",
      "  평균: 151.43 ± 56.89\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"데이터 전처리 통계 요약 (IQR 1회만 적용)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n원본 데이터:\")\n",
    "print(f\"  개수: {len(df):,}\")\n",
    "print(f\"  범위: [{df['value'].min():.2f}, {df['value'].max():.2f}]\")\n",
    "print(f\"  평균: {df['value'].mean():.2f} ± {df['value'].std():.2f}\")\n",
    "\n",
    "print(f\"\\nIQR 후 (최종):\")\n",
    "print(f\"  개수: {len(df_filtered):,}\")\n",
    "print(f\"  제거: {len(df) - len(df_filtered):,}개 ({(len(df)-len(df_filtered))/len(df)*100:.2f}%)\")\n",
    "print(f\"  범위: [{df_filtered['value'].min():.2f}, {df_filtered['value'].max():.2f}]\")\n",
    "print(f\"  평균: {df_filtered['value'].mean():.2f} ± {df_filtered['value'].std():.2f}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f6649",
   "metadata": {},
   "source": [
    "3. Slding Windows 생성\n",
    "슬라이딩 윈도우를 사용하여 dataset\n",
    "\n",
    "X[i : i+60] : 과거 60분간의 연속된 데이터 (예: 00:00 ~ 00:59).\n",
    "y[i+60 : (i+60)+10] : 이후 10분간의 예측 목표 데이터 (예: 01:00 ~ 01:09).\n",
    "\n",
    "time = index\n",
    "value = idx[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe1668f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 슬라이딩 윈도우 생성 ===\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n=== 슬라이딩 윈도우 생성 ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc893d7",
   "metadata": {},
   "source": [
    "- ascontiguousarray 동작. 불연속 -> 연속\n",
    "sliding_window_view의 view를 실제 복사로 전환\n",
    "새 메모리: [0,1,2, 1,2,3, 2,3,4, 3,4,5]\n",
    "            ─────  ─────  ─────  ─────\n",
    "            X[0]   X[1]   X[2]   X[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb418e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X shape: (943281, 144, 1)\n",
      "y shape: (943281, 10)\n"
     ]
    }
   ],
   "source": [
    "def create_sliding_windows(data, input_time, output_time):\n",
    "\n",
    "    \"\"\" \n",
    "    슬라이딩 윈도우 생성 (y는 2D로)\n",
    "\n",
    "    Ags:\n",
    "        data: 1D numpy array\n",
    "        input_time : 윈도우 크기 \n",
    "        output_time: 출력 윈도우 크기\n",
    "\n",
    "    Returns:\n",
    "        X: (n_samples, input_time, 1) - 3D\n",
    "        y: (n_samples, output_time) - 2D\n",
    "    \"\"\"\n",
    "    \n",
    "    X, y = [], []\n",
    "    n=len(data)\n",
    "    n_samples = n - input_time - output_time + 1\n",
    "\n",
    "    # 메모리 복사 없이 **view**만 생성 (stride 조작). 실제복사 X. 포인터만 참조\n",
    "    X = np.lib.stride_tricks.sliding_window_view(data, input_time)[:n_samples]\n",
    "\n",
    "    # 핵심: **`data[input_time:]`** → X의 각 윈도우가 끝난 **직후** 시점부터 시작\n",
    "    y = np.lib.stride_tricks.sliding_window_view(data[input_time:], output_time)[:n_samples]\n",
    "\n",
    "    # `ascontiguousarray`: stride_tricks가 만든 view는 메모리가 불연속적일 수 있어서 연속 배열로 복사\n",
    "    X = np.ascontiguousarray(X).reshape(-1, input_time, 1)\n",
    "    y = np.ascontiguousarray(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# 정규화 전에 슬라이딩 윈도우 생성\n",
    "value_numpy = df_filtered['value'].values # tablel에서 value 값만 get.\n",
    "\n",
    "X, y = create_sliding_windows(value_numpy, input_time, output_time)\n",
    "\n",
    "# LSTM에 사용하기 위해 뒤에 1차원 추가\n",
    "# X_reshape=X.reshape(X.shape[0], X.shape[1], 1)\n",
    "# y_reshpae=y.reshape(y.shape[0], y.shape[1], 1)\n",
    "\n",
    "# X_reshape = X[:, :, np.newaxis]\n",
    "# y_reshape = y[:, :, np.newaxis]\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "if len(X) == 0:\n",
    "    raise ValueError(\"슬라이딩 윈도우 생성 실패!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f494383d",
   "metadata": {},
   "source": [
    "4. 데이터 분할\n",
    "- Train, Validation, Test Split (0.7/0.15/0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18657b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train/Validation/Test Split ===\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n=== Train/Validation/Test Split ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f1ba819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 660296개 (70%)\n",
      "Validation: 141492개 (15%)\n",
      "Test: 141493개 (15%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "n_samples = len(X)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "train_idx = int(n_samples * train_ratio)\n",
    "val_idx = int(n_samples * (train_ratio + val_ratio))\n",
    "\n",
    "X_train = X[:train_idx]\n",
    "y_train = y[:train_idx]\n",
    "\n",
    "X_val = X[train_idx:val_idx]\n",
    "y_val = y[train_idx:val_idx]\n",
    "\n",
    "X_test = X[val_idx:]\n",
    "y_test = y[val_idx:]\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]}개 ({train_ratio*100:.0f}%)\")\n",
    "print(f\"Validation: {X_val.shape[0]}개 ({val_ratio*100:.0f}%)\")\n",
    "print(f\"Test: {X_test.shape[0]}개 ({test_ratio*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddc678",
   "metadata": {},
   "source": [
    "샘플 50,000개로 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf6b85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_samples = 50000\n",
    "# step = len(X_train) // max_samples\n",
    "\n",
    "# X_train = X_train[::step][:max_samples]\n",
    "# y_train = y_train[::step][:max_samples]\n",
    "\n",
    "# print(f\"테스트용 Train 샘플: {len(X_train)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76beef07",
   "metadata": {},
   "source": [
    "5. MinMaxScaler 데이터 정규화 (Train으로만 Fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a1fdd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 정규화 (Train 기준) ===\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n=== 정규화 (Train 기준) ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0c06ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train 데이터로만 Fit 완료\n",
      "Scaler 범위: [0.00, 323.74]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Train 데이터로만 Scaler Fit\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train.reshape(-1,1))\n",
    "\n",
    "# min/max 추출 -> reshape 없이 직접 연산\n",
    "data_min = scaler.data_min_[0]\n",
    "data_max = scaler.data_max_[0]\n",
    "\n",
    "def normalize(arr): # 정규화\n",
    "    return (arr - data_min) / (data_max - data_min)\n",
    "\n",
    "def denormalize(arr): # 역변환\n",
    "    return arr * (data_max - data_min) + data_min\n",
    "\n",
    "# 1. reshape(-1, 1) ->2D배열  / 2. X_ .shape 으로 변경\n",
    "# 각 세트 Transform\n",
    "X_train_scaled = normalize(X_train)\n",
    "X_val_scaled = normalize(X_val)\n",
    "X_test_scaled = normalize(X_test)\n",
    "\n",
    "# y도 같은 Scaler로 Transform\n",
    "y_train_scaled = normalize(y_train)\n",
    "y_val_scaled = normalize(y_val)\n",
    "y_test_scaled = normalize(y_test)\n",
    "\n",
    "print(f\"✅ Train 데이터로만 Fit 완료\")\n",
    "print(f\"Scaler 범위: [{data_min:.2f}, {data_max:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3542c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_numpy = value_numpy[:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaeed70",
   "metadata": {},
   "source": [
    "- Tensor 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b011efe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "\n",
    "# 텐서 변환\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train_scaled).to(device)\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val_scaled).to(device)\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test_scaled).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160339de",
   "metadata": {},
   "source": [
    "- DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61e833ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DataLoader ===\n",
      "Batch size: 256\n",
      "Val/Test batch size: 64\n",
      "Train batches: 2579\n",
      "Validation batches: 277\n",
      "Test batches: 277\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size=256\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# 각 샘플은 이미 \"완성된 시퀀스\" \"X[100], y[100]\" 이므로 shuffle=True ok\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=4,\n",
    "    pin_memory=False,\n",
    "    drop_last=True, \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=512, #no_Grad (중간 활성화값 저장 X , gradient 텐서 생성 X)\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")                       # 검증/테스트에서는 전체 데이터를 평가해야 함\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=512, \n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\\n=== DataLoader ===\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Val/Test batch size: 64\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac3e42c",
   "metadata": {},
   "source": [
    "6. LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0c37318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 모델 ===\n",
      "총 파라미터: 200,458개\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, output_size=1440, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    # x: (batch, 60, 1) 60분 input\n",
    "    # out, (h_n, c_n) = self.lstm(x) # 모든 시간 단계의 hidden_state, (마지막hidden_cell, 마지막cell_state)\n",
    "    # last_hidden = h_n[-1] # (batch, hidden_size) # \"최종 은닉 상태 사용\"\n",
    "    # out = self.fc(last_hidden) # (batch, 10) # 10분 예측\n",
    "    # return out.unsqueeze(-1) # (batch, 10, 1) to match y\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            x: (batch, 60, 1) - 60분 입력 데이터\n",
    "        Returns:\n",
    "            (batch, 10, 1) - 10분 예측값\n",
    "        \"\"\"\n",
    "        _, (hidden_cell, _) = self.lstm(x) # (batch, 60, 64)\n",
    "        last_hidden = hidden_cell[-1] # (batch, 64) # 최종 은닉상태 사용\n",
    "        out = self.fc(last_hidden) # (batch, 10) # 10분 예측\n",
    "        return out # (batch, 10)    # check\n",
    "    \n",
    "model = LSTMModel(\n",
    "    input_size=1,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    output_size = output_time,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n=== 모델 ===\")\n",
    "print(f\"총 파라미터: {total_params:,}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c23e6",
   "metadata": {},
   "source": [
    "6. 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6513aa0",
   "metadata": {},
   "source": [
    "학습 함수 필수 사항\n",
    "1. Loss Function: criterion = nn.MSELoss()\n",
    "2. Optimizer: optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "3. Train Loop:\n",
    "   - model.train()\n",
    "   - forward → loss → backward → step\n",
    "4. Eval Loop:\n",
    "   - model.eval()\n",
    "   - with torch.no_grad()\n",
    "5. 손실 기록: train_losses, test_losses\n",
    "\n",
    "선택 사항\n",
    "- DataLoader (배치 처리)\n",
    "- Learning Rate Scheduler (학습률 조정)\n",
    "- Early Stopping (과적합 방지)\n",
    "- Gradient Clipping (gradient 폭발 방지)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af08d60",
   "metadata": {},
   "source": [
    "7. Early Stopping 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "078281d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=1e-5, verbose=True): #일정 크기 개선되어야 '개선'으로 인정\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        patience: validataion loss가 개선되지 않아도 기다릴 epoch 수\n",
    "        min_delta: 개선으로 인정할 최소 변화량\n",
    "        verbose: 로그 출력 여부\n",
    "        \"\"\"     \n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss: float | None = None\n",
    "        self.early_stop = False\n",
    "        self.best_model: dict[str, torch.Tensor] | None = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "\n",
    "        elif val_loss < self.best_loss - self.min_delta: # 의미 있는 개선으로 인정\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        \n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        ''' validation loss가 감소하면 모델 저장'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f}). Saving model...')\n",
    "        self.best_model = model.state_dict().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f9578",
   "metadata": {},
   "source": [
    "8. 학습\n",
    "8-1. 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdfd416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 학습 설정 ===\n",
      "Max Epochs: 100\n",
      "Early Stopping Patience: 7\n",
      "Learning Rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 7\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "train_losses=[]\n",
    "val_losses=[]\n",
    "\n",
    "print(f\"\\n=== 학습 설정 ===\")\n",
    "print(f\"Max Epochs: {num_epochs}\")\n",
    "print(f\"Early Stopping Patience: {patience}\")\n",
    "print(f\"Learning Rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5760a3e7",
   "metadata": {},
   "source": [
    "8-2. 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Started(with Early Stopping)\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/usr/lib/python3.14/multiprocessing/forkserver.py\"\u001b[0m, line \u001b[35m344\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    code = _serve_one(child_r, fds,\n",
      "                      unused_fds,\n",
      "                      old_handlers)\n",
      "  File \u001b[35m\"/usr/lib/python3.14/multiprocessing/forkserver.py\"\u001b[0m, line \u001b[35m384\u001b[0m, in \u001b[35m_serve_one\u001b[0m\n",
      "    code = spawn._main(child_r, parent_sentinel)\n",
      "  File \u001b[35m\"/usr/lib/python3.14/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \u001b[35m\"/home/user/314env/lib/python3.14/site-packages/torch/multiprocessing/reductions.py\"\u001b[0m, line \u001b[35m180\u001b[0m, in \u001b[35mrebuild_cuda_tensor\u001b[0m\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "        storage_device,\n",
      "    ...<6 lines>...\n",
      "        event_sync_required,\n",
      "    )\n",
      "  File \u001b[35m\"/home/user/314env/lib/python3.14/site-packages/torch/storage.py\"\u001b[0m, line \u001b[35m1455\u001b[0m, in \u001b[35m_new_shared_cuda\u001b[0m\n",
      "    return \u001b[31mtorch.UntypedStorage._new_shared_cuda\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/usr/lib/python3.14/multiprocessing/forkserver.py\"\u001b[0m, line \u001b[35m344\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    code = _serve_one(child_r, fds,\n",
      "                      unused_fds,\n",
      "                      old_handlers)\n",
      "  File \u001b[35m\"/usr/lib/python3.14/multiprocessing/forkserver.py\"\u001b[0m, line \u001b[35m384\u001b[0m, in \u001b[35m_serve_one\u001b[0m\n",
      "    code = spawn._main(child_r, parent_sentinel)\n",
      "  File \u001b[35m\"/usr/lib/python3.14/multiprocessing/forkserver.py\"\u001b[0m, line \u001b[35m344\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    code = _serve_one(child_r, fds,\n",
      "                      unused_fds,\n",
      "                      old_handlers)\n",
      "  File \u001b[35m\"/usr/lib/python3.14/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \u001b[35m\"/home/user/314env/lib/python3.14/site-packages/torch/multiprocessing/reductions.py\"\u001b[0m, line \u001b[35m180\u001b[0m, in \u001b[35mrebuild_cuda_tensor\u001b[0m\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "        storage_device,\n",
      "    ...<6 lines>...\n",
      "        event_sync_required,\n",
      "    )\n",
      "  File \u001b[35m\"/usr/lib/python3.14/multiprocessing/forkserver.py\"\u001b[0m, line \u001b[35m384\u001b[0m, in \u001b[35m_serve_one\u001b[0m\n",
      "    code = spawn._main(child_r, parent_sentinel)\n",
      "\u001b[1;35mtorch.AcceleratorError\u001b[0m: \u001b[35mCUDA error: invalid resource handle\n",
      "Search for `cudaErrorInvalidResourceHandle' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[0m\n",
      "  File \u001b[35m\"/usr/lib/python3.14/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \u001b[35m\"/home/user/314env/lib/python3.14/site-packages/torch/storage.py\"\u001b[0m, line \u001b[35m1455\u001b[0m, in \u001b[35m_new_shared_cuda\u001b[0m\n",
      "    return \u001b[31mtorch.UntypedStorage._new_shared_cuda\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/user/314env/lib/python3.14/site-packages/torch/multiprocessing/reductions.py\"\u001b[0m, line \u001b[35m180\u001b[0m, in \u001b[35mrebuild_cuda_tensor\u001b[0m\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "        storage_device,\n",
      "    ...<6 lines>...\n",
      "        event_sync_required,\n",
      "    )\n",
      "  File \u001b[35m\"/home/user/314env/lib/python3.14/site-packages/torch/storage.py\"\u001b[0m, line \u001b[35m1455\u001b[0m, in \u001b[35m_new_shared_cuda\u001b[0m\n",
      "    return \u001b[31mtorch.UntypedStorage._new_shared_cuda\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/usr/lib/python3.14/multiprocessing/forkserver.py\"\u001b[0m, line \u001b[35m344\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    code = _serve_one(child_r, fds,\n",
      "                      unused_fds,\n",
      "                      old_handlers)\n",
      "  File \u001b[35m\"/usr/lib/python3.14/multiprocessing/forkserver.py\"\u001b[0m, line \u001b[35m384\u001b[0m, in \u001b[35m_serve_one\u001b[0m\n",
      "    code = spawn._main(child_r, parent_sentinel)\n",
      "\u001b[1;35mtorch.AcceleratorError\u001b[0m: \u001b[35mCUDA error: invalid resource handle\n",
      "Search for `cudaErrorInvalidResourceHandle' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[0m\n",
      "  File \u001b[35m\"/usr/lib/python3.14/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \u001b[35m\"/home/user/314env/lib/python3.14/site-packages/torch/multiprocessing/reductions.py\"\u001b[0m, line \u001b[35m180\u001b[0m, in \u001b[35mrebuild_cuda_tensor\u001b[0m\n",
      "    storage = storage_cls._new_shared_cuda(\n",
      "        storage_device,\n",
      "    ...<6 lines>...\n",
      "        event_sync_required,\n",
      "    )\n",
      "\u001b[1;35mtorch.AcceleratorError\u001b[0m: \u001b[35mCUDA error: invalid resource handle\n",
      "Search for `cudaErrorInvalidResourceHandle' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[0m\n",
      "  File \u001b[35m\"/home/user/314env/lib/python3.14/site-packages/torch/storage.py\"\u001b[0m, line \u001b[35m1455\u001b[0m, in \u001b[35m_new_shared_cuda\u001b[0m\n",
      "    return \u001b[31mtorch.UntypedStorage._new_shared_cuda\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mtorch.AcceleratorError\u001b[0m: \u001b[35mCUDA error: invalid resource handle\n",
      "Search for `cudaErrorInvalidResourceHandle' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 77433, 77434, 77435, 77436) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/314env/lib/python3.14/site-packages/torch/utils/data/dataloader.py:1310\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.14/multiprocessing/queues.py:112\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m model.train()\n\u001b[32m     14\u001b[39m train_loss_epoch = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# (batch, 10)\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/314env/lib/python3.14/site-packages/torch/utils/data/dataloader.py:741\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    747\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/314env/lib/python3.14/site-packages/torch/utils/data/dataloader.py:1524\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1520\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding <= \u001b[32m0\u001b[39m:\n\u001b[32m   1521\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m   1522\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInvalid iterator state: shutdown or no outstanding tasks when fetching next data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1523\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1524\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1525\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1527\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/314env/lib/python3.14/site-packages/torch/utils/data/dataloader.py:1483\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1480\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1482\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1483\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1484\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1485\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/314env/lib/python3.14/site-packages/torch/utils/data/dataloader.py:1323\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1322\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1323\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1324\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1325\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 77433, 77434, 77435, 77436) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Started(with Early Stopping)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    # ======= Training =======\n",
    "    model.train()\n",
    "    train_loss_epoch = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(batch_X) # (batch, 10)\n",
    "        loss = criterion(outputs, batch_y) \n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad() # 기울기 초기화\n",
    "        loss.backward() # 손실함수의 각 파라미터에 대한 미분값 게산\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step() # 계산된gradient를 사용하여 실제 파라미터 값 업데이트\n",
    "        train_loss_epoch += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # ====== validation ======\n",
    "    model.eval()\n",
    "    val_loss_epoch = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss_epoch += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss_epoch / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "\n",
    "    # 출력\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1:3d}/{num_epochs}] '\n",
    "              f'Train: {avg_train_loss:.6f} '\n",
    "              f'Val: {avg_val_loss:.6f} '\n",
    "              f'Time: {epoch_time:.1f}s')\n",
    "    \n",
    "    # ===== Early Stopping Check ========\n",
    "    early_stopping(avg_val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\n⭐ Early Stopping at Epoch {epoch+1}\")\n",
    "        print(f\"Best Validation Loss: {early_stopping.best_loss:.6f}\")\n",
    "        break\n",
    "\n",
    "# Best 모델 로드\n",
    "if early_stopping.best_model is not None:\n",
    "    model.load_state_dict(early_stopping.best_model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Completed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508ab482",
   "metadata": {},
   "source": [
    "9. Test 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c839781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== Test Evaluation ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "model.eval()\n",
    "test_loss_epoch = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_predictions = []\n",
    "    test_actuals = []\n",
    "\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss_epoch += loss.item()\n",
    "\n",
    "        test_predictions.append(outputs.cpu().numpy())\n",
    "        test_actuals.append(batch_y.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss_epoch / len(test_loader)\n",
    "test_predictions = np.vstack(test_predictions)\n",
    "test_actuals = np.vstack(test_actuals)\n",
    "\n",
    "# 역 정규화 (denormalize 함수 사용)\n",
    "test_pred_original = denormalize(test_predictions)\n",
    "test_actual_original = denormalize(test_actuals)\n",
    "\n",
    "# 평가 지표 (원본 스케일)\n",
    "test_rmse = np.sqrt(mean_squared_error(test_actual_original, test_pred_original))\n",
    "test_mae = mean_absolute_error(test_actual_original, test_pred_original)\n",
    "\n",
    "print(f\"Test Loss (Normalized): {avg_test_loss:.6f}\")\n",
    "print(f\"Test RMSE (Original Scale): {test_rmse:.4f}\")\n",
    "print(f\"Test MAE (Original Scale): {test_mae:.4f}\")\n",
    "\n",
    "# 기존 RMSE, MAE 코드 아래에 추가\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "test_mape = mean_absolute_percentage_error(test_actual_original.flatten(), \n",
    "                                            test_pred_original.flatten())\n",
    "print(f\"Test MAPE: {test_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae3b91",
   "metadata": {},
   "source": [
    "- 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ce763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "plt.axvline(len(train_losses)-1, color='red', linestyle='--', alpha=0.5, \n",
    "           label=f'Stopped at Epoch {len(train_losses)}')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training History', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training History (Log Scale)', fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4832f1",
   "metadata": {},
   "source": [
    "- 예측 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999dcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "actual = test_actual_original[sample_idx]\n",
    "predicted = test_pred_original[sample_idx]\n",
    "minutes = np.arange(output_time)  # 분 단위\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(minutes, actual, label='Actual', \n",
    "         linewidth=2, marker='o', markersize=5, alpha=0.7, color='blue')\n",
    "plt.plot(minutes, predicted, label='Predicted', \n",
    "         linewidth=2, marker='s', markersize=5, alpha=0.7, color='red')\n",
    "plt.xlabel('Time (Minutes)', fontsize=12)\n",
    "plt.ylabel('Value', fontsize=12)\n",
    "plt.title(f'10-Min Prediction (Sample {sample_idx})', \n",
    "          fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad41a3",
   "metadata": {},
   "source": [
    "- 최종 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Data Split: Train {train_ratio*100:.0f}% / Val {val_ratio*100:.0f}% / Test {test_ratio*100:.0f}%\")\n",
    "print(f\"Stopped at Epoch: {len(train_losses)}\")\n",
    "print(f\"Best Val Loss: {early_stopping.best_loss:.6f}\")\n",
    "print(f\"Test Loss: {avg_test_loss:.6f}\")\n",
    "print(f\"Test RMSE (Original): {test_rmse:.4f}\")\n",
    "print(f\"Test MAE (Original): {test_mae:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b553e6",
   "metadata": {},
   "source": [
    "- 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'data_min': data_min,\n",
    "    'data_max': data_max,   # scaler 대신 min/max 저장\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'test_loss': avg_test_loss,\n",
    "    'best_epoch': len(train_losses),\n",
    "    'input_time': input_time,\n",
    "    'output_time': output_time,\n",
    "}, 'lstm_5days_to_1day_improved.pth')\n",
    "\n",
    "print(\"\\n✅ 모델 저장 완료: lstm_5days_to_1day_improved.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36797383",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12), sharey=True)\n",
    "sample_indices = [0, 100, 500, 1000, 5000, 10000, 20000, 50000, -1]\n",
    "\n",
    "for idx, ax in zip(sample_indices, axes.flatten()):\n",
    "    actual = test_actual_original[idx]\n",
    "    predicted = test_pred_original[idx]\n",
    "    minutes = np.arange(output_time)\n",
    "    \n",
    "    ax.plot(minutes, actual, label='Actual', linewidth=2, color='blue')\n",
    "    ax.plot(minutes, predicted, label='Predicted', linewidth=2, color='red')\n",
    "    ax.set_title(f'Sample {idx}', fontsize=10, fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Multi-Sample Prediction Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a7953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = test_actual_original - test_pred_original  # (n_samples, output_time)\n",
    "\n",
    "step_rmse = np.sqrt(np.mean(errors**2, axis=0))\n",
    "step_mae = np.mean(np.abs(errors), axis=0)\n",
    "\n",
    "minutes = np.arange(output_time)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(minutes, step_rmse, color='coral')\n",
    "axes[0].set_xlabel('Prediction Step (min)')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE by Prediction Step')\n",
    "\n",
    "axes[1].bar(minutes, step_mae, color='steelblue')\n",
    "axes[1].set_xlabel('Prediction Step (min)')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE by Prediction Step')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0486bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errors = (test_actual_original - test_pred_original).flatten()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(all_errors, bins=100, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(0, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('Error (Actual - Predicted)')\n",
    "axes[0].set_title('Error Distribution')\n",
    "\n",
    "# 편향 확인\n",
    "mean_error = np.mean(all_errors)\n",
    "axes[1].boxplot(all_errors)\n",
    "axes[1].set_title(f'Error Boxplot (Mean Bias: {mean_error:.2f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Error (Bias): {mean_error:.4f}\")\n",
    "print(f\"Std Error: {np.std(all_errors):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f99b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제값 크기별 MAPE\n",
    "actuals_flat = test_actual_original.flatten()\n",
    "preds_flat = test_pred_original.flatten()\n",
    "\n",
    "bins = np.percentile(actuals_flat, [0, 25, 50, 75, 100])\n",
    "labels = ['Q1(Low)', 'Q2', 'Q3', 'Q4(High)']\n",
    "\n",
    "for i in range(len(bins)-1):\n",
    "    mask = (actuals_flat >= bins[i]) & (actuals_flat < bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        segment_mape = np.mean(np.abs((actuals_flat[mask] - preds_flat[mask]) / actuals_flat[mask])) * 100\n",
    "        segment_rmse = np.sqrt(np.mean((actuals_flat[mask] - preds_flat[mask])**2))\n",
    "        print(f\"{labels[i]}: MAPE={segment_mape:.2f}%, RMSE={segment_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ih7x1gq2isr",
   "metadata": {},
   "source": [
    "- Timetable 그래프 (시간축 예측 결과)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fubmo11b0p",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "# 테스트 구간의 시간 인덱스 복원 (numpy datetime64로 명시 변환)\n",
    "test_start = val_idx + input_time\n",
    "test_times = pd.to_datetime(df_filtered['time'].iloc[test_start : test_start + len(test_actual_original)]).to_numpy()\n",
    "\n",
    "# 실제값: 각 샘플의 첫 번째 예측값으로 연속 시계열 구성\n",
    "actual_series = test_actual_original[:, 0]\n",
    "pred_series = test_pred_original[:, 0]\n",
    "\n",
    "# === 1. 전체 테스트 구간 Timetable ===\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "ax.plot(test_times, actual_series, label='Actual', linewidth=1.2, color='#2196F3', alpha=0.8)\n",
    "ax.plot(test_times, pred_series, label='Predicted', linewidth=1.2, color='#F44336', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Time', fontsize=12)\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title('Test Period Timetable: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "ax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 2. 1주일 단위 확대 Timetable (마지막 7일) ===\n",
    "samples_per_day = 60 * 24  # 1분 간격 = 1440 samples/day\n",
    "last_7days = 7 * samples_per_day\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "times_zoom = test_times[-last_7days:]\n",
    "actual_zoom = actual_series[-last_7days:]\n",
    "pred_zoom = pred_series[-last_7days:]\n",
    "\n",
    "ax.plot(times_zoom, actual_zoom, label='Actual', linewidth=1.5, color='#2196F3')\n",
    "ax.plot(times_zoom, pred_zoom, label='Predicted', linewidth=1.5, color='#F44336', alpha=0.8)\n",
    "ax.fill_between(times_zoom, actual_zoom, pred_zoom, alpha=0.15, color='gray', label='Error')\n",
    "\n",
    "ax.set_xlabel('Time', fontsize=12)\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title('Timetable (Last 7 Days) - Zoom In', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))\n",
    "ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 3. 1일 단위 확대 Timetable (마지막 24시간) ===\n",
    "last_1day = samples_per_day\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "times_1d = test_times[-last_1day:]\n",
    "actual_1d = actual_series[-last_1day:]\n",
    "pred_1d = pred_series[-last_1day:]\n",
    "\n",
    "ax.plot(times_1d, actual_1d, label='Actual', linewidth=2, marker='.', markersize=2, color='#2196F3')\n",
    "ax.plot(times_1d, pred_1d, label='Predicted', linewidth=2, marker='.', markersize=2, color='#F44336', alpha=0.8)\n",
    "ax.fill_between(times_1d, actual_1d, pred_1d, alpha=0.2, color='orange', label='Error')\n",
    "\n",
    "ax.set_xlabel('Time', fontsize=12)\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title('Timetable (Last 24 Hours) - Detail View', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "ax.xaxis.set_major_locator(mdates.HourLocator(interval=2))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Timetable 요약 ===\")\n",
    "print(f\"테스트 기간: {pd.Timestamp(test_times[0])} ~ {pd.Timestamp(test_times[-1])}\")\n",
    "print(f\"총 샘플 수: {len(test_times):,}개\")\n",
    "print(f\"RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, MAPE: {test_mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "314env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
